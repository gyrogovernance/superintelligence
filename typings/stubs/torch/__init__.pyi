from collections.abc import Iterator, Sequence
from typing import Any

class dtype:
    name: str

class device:
    type: str
    index: int | None
    def __init__(self, device: device | str | int = "cpu") -> None: ...

class Tensor:
    @property
    def dtype(self) -> dtype: ...
    @property
    def device(self) -> device: ...
    @property
    def shape(self) -> tuple[int, ...]: ...
    @property
    def ndim(self) -> int: ...
    @property
    def T(self) -> Tensor: ...
    @property
    def real(self) -> Tensor: ...
    @property
    def imag(self) -> Tensor: ...
    @property
    def requires_grad(self) -> bool: ...
    @property
    def data(self) -> Tensor: ...
    @property
    def grad(self) -> Tensor | None: ...

    def size(self, dim: int | None = None) -> Any: ...
    def dim(self) -> int: ...
    def numel(self) -> int: ...

    def view(self, *shape: int) -> Tensor: ...
    def reshape(self, *shape: int) -> Tensor: ...
    def flatten(self, start_dim: int = 0, end_dim: int = -1) -> Tensor: ...
    def unsqueeze(self, dim: int) -> Tensor: ...
    def squeeze(self, dim: int | None = None) -> Tensor: ...
    def transpose(self, dim0: int, dim1: int) -> Tensor: ...
    def permute(self, *dims: int) -> Tensor: ...
    def contiguous(self) -> Tensor: ...
    def is_contiguous(self) -> bool: ...
    def stride(self, dim: int | None = None) -> Any: ...
    def storage_offset(self) -> int: ...
    def expand(self, *sizes: int) -> Tensor: ...
    def repeat(self, *sizes: int) -> Tensor: ...

    def to(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def cpu(self) -> Tensor: ...
    def cuda(self, device: Any | None = None) -> Tensor: ...
    def type(self, dtype: Any = None) -> Any: ...

    def float(self) -> Tensor: ...
    def half(self) -> Tensor: ...
    def bfloat16(self) -> Tensor: ...
    def double(self) -> Tensor: ...
    def long(self) -> Tensor: ...
    def int(self) -> Tensor: ...
    def bool(self) -> Tensor: ...

    def item(self) -> Any: ...
    def numpy(self) -> Any: ...
    def tolist(self) -> Any: ...

    def pow(self, exponent: Any) -> Tensor: ...
    def sqrt(self) -> Tensor: ...
    def abs(self) -> Tensor: ...
    def exp(self) -> Tensor: ...
    def log(self) -> Tensor: ...
    def sin(self) -> Tensor: ...
    def cos(self) -> Tensor: ...
    def tanh(self) -> Tensor: ...
    def sigmoid(self) -> Tensor: ...
    def clamp(self, min: float | None = None, max: float | None = None) -> Tensor: ...
    def clip(self, min: float | None = None, max: float | None = None) -> Tensor: ...

    def mean(self, dim: Any | None = None, keepdim: bool = False) -> Tensor: ...
    def sum(self, dim: Any | None = None, keepdim: bool = False) -> Tensor: ...
    def std(self, dim: Any | None = None, keepdim: bool = False, unbiased: bool = True) -> Tensor: ...
    def var(self, dim: Any | None = None, keepdim: bool = False, unbiased: bool = True) -> Tensor: ...
    def norm(self, p: Any = 2, dim: Any | None = None, keepdim: bool = False) -> Tensor: ...
    def max(self, dim: int | None = None, keepdim: bool = False) -> Any: ...
    def min(self, dim: int | None = None, keepdim: bool = False) -> Any: ...
    def argmax(self, dim: int | None = None, keepdim: bool = False) -> Tensor: ...
    def argmin(self, dim: int | None = None, keepdim: bool = False) -> Tensor: ...
    def topk(self, k: int, dim: int = -1, largest: bool = True, sorted: bool = True) -> tuple[Tensor, Tensor]: ...
    def sort(self, dim: int = -1, descending: bool = False) -> tuple[Tensor, Tensor]: ...

    def __iter__(self) -> Iterator[Tensor]: ...
    def __len__(self) -> int: ...
    def __getitem__(self, key: Any) -> Tensor: ...
    def __setitem__(self, key: Any, value: Any) -> None: ...
    def __bool__(self) -> bool: ...
    def __int__(self) -> int: ...
    def __float__(self) -> float: ...

    def __matmul__(self, other: Tensor) -> Tensor: ...
    def __rmatmul__(self, other: Tensor) -> Tensor: ...
    def matmul(self, other: Tensor) -> Tensor: ...
    def mm(self, mat2: Tensor) -> Tensor: ...
    def bmm(self, mat2: Tensor) -> Tensor: ...
    def mv(self, vec: Tensor) -> Tensor: ...
    def dot(self, other: Tensor) -> Tensor: ...

    def __mul__(self, other: Any) -> Tensor: ...
    def __rmul__(self, other: Any) -> Tensor: ...
    def __add__(self, other: Any) -> Tensor: ...
    def __radd__(self, other: Any) -> Tensor: ...
    def __sub__(self, other: Any) -> Tensor: ...
    def __rsub__(self, other: Any) -> Tensor: ...
    def __truediv__(self, other: Any) -> Tensor: ...
    def __rtruediv__(self, other: Any) -> Tensor: ...
    def __floordiv__(self, other: Any) -> Tensor: ...
    def __rfloordiv__(self, other: Any) -> Tensor: ...
    def __mod__(self, other: Any) -> Tensor: ...
    def __pow__(self, other: Any) -> Tensor: ...
    def __rpow__(self, other: Any) -> Tensor: ...
    def __neg__(self) -> Tensor: ...
    def __pos__(self) -> Tensor: ...
    def __abs__(self) -> Tensor: ...
    def __invert__(self) -> Tensor: ...
    def __xor__(self, other: Any) -> Tensor: ...
    def __and__(self, other: Any) -> Tensor: ...
    def __or__(self, other: Any) -> Tensor: ...

    def __gt__(self, other: Any) -> Tensor: ...
    def __lt__(self, other: Any) -> Tensor: ...
    def __ge__(self, other: Any) -> Tensor: ...
    def __le__(self, other: Any) -> Tensor: ...
    def __eq__(self, other: Any) -> Tensor: ...  # type: ignore[override]
    def __ne__(self, other: Any) -> Tensor: ...  # type: ignore[override]

    def clone(self) -> Tensor: ...
    def detach(self) -> Tensor: ...
    def requires_grad_(self, requires_grad: bool = True) -> Tensor: ...
    def backward(self, gradient: Tensor | None = None, retain_graph: bool | None = None) -> None: ...

    def copy_(self, src: Tensor) -> Tensor: ...
    def fill_(self, value: Any) -> Tensor: ...
    def zero_(self) -> Tensor: ...
    def add_(self, other: Any) -> Tensor: ...
    def sub_(self, other: Any) -> Tensor: ...
    def mul_(self, other: Any) -> Tensor: ...
    def div_(self, other: Any) -> Tensor: ...

    def repeat_interleave(self, repeats: Any, dim: int | None = None) -> Tensor: ...
    def chunk(self, chunks: int, dim: int = 0) -> list[Tensor]: ...
    def split(self, split_size_or_sections: Any, dim: int = 0) -> list[Tensor]: ...
    def unbind(self, dim: int = 0) -> tuple[Tensor, ...]: ...

    def allclose(self, other: Tensor, rtol: float = 1e-05, atol: float = 1e-08, equal_nan: bool = False) -> bool: ...
    def eq(self, other: Any) -> Tensor: ...
    def ne(self, other: Any) -> Tensor: ...
    def masked_fill(self, mask: Tensor, value: Any) -> Tensor: ...
    def masked_fill_(self, mask: Tensor, value: Any) -> Tensor: ...
    def where(self, condition: Tensor, other: Tensor) -> Tensor: ...

    def element_size(self) -> int: ...
    def is_cuda(self) -> bool: ...
    def is_contiguous(self) -> bool: ...
    def is_floating_point(self) -> bool: ...
    def is_complex(self) -> bool: ...

    # For dynamic attribute access
    def __getattr__(self, name: str) -> Any: ...


# --- dtype singletons ---
uint8: dtype
int8: dtype
int16: dtype
int32: dtype
int64: dtype
float16: dtype
float32: dtype
float64: dtype
bfloat16: dtype
complex64: dtype
complex128: dtype
bool: dtype  # type: ignore[assignment]

# Common aliases
half: dtype
float: dtype  # type: ignore[assignment]
double: dtype
long: dtype
int: dtype  # type: ignore[assignment]
short: dtype
cfloat: dtype
cdouble: dtype


# --- Tensor creation ---
def tensor(data: Any, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def as_tensor(data: Any, dtype: dtype | None = None, device: Any | None = None) -> Tensor: ...
def from_numpy(ndarray: Any) -> Tensor: ...

def zeros(*size: Any, out: Tensor | None = None, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def ones(*size: Any, out: Tensor | None = None, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def empty(*size: Any, out: Tensor | None = None, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def full(size: Any, fill_value: Any, out: Tensor | None = None, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def zeros_like(input: Tensor, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def ones_like(input: Tensor, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def empty_like(input: Tensor, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def full_like(input: Tensor, fill_value: Any, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...

def arange(start: Any, end: Any | None = None, step: Any = 1, out: Tensor | None = None, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def linspace(start: Any, end: Any, steps: int, out: Tensor | None = None, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def logspace(start: Any, end: Any, steps: int, base: float = 10.0, out: Tensor | None = None, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def eye(n: int, m: int | None = None, out: Tensor | None = None, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...

def rand(*size: Any, out: Tensor | None = None, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def randn(*size: Any, out: Tensor | None = None, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def randint(low: int, high: int | None = None, size: Any | None = None, generator: Any | None = None, out: Tensor | None = None, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...
def randperm(n: int, out: Tensor | None = None, dtype: dtype | None = None, device: Any | None = None, requires_grad: bool = False) -> Tensor: ...


# --- Tensor operations ---
def cat(tensors: Sequence[Tensor], dim: int = 0, out: Tensor | None = None) -> Tensor: ...
def concat(tensors: Sequence[Tensor], dim: int = 0, out: Tensor | None = None) -> Tensor: ...
def stack(tensors: Sequence[Tensor], dim: int = 0, out: Tensor | None = None) -> Tensor: ...
def vstack(tensors: Sequence[Tensor], out: Tensor | None = None) -> Tensor: ...
def hstack(tensors: Sequence[Tensor], out: Tensor | None = None) -> Tensor: ...
def dstack(tensors: Sequence[Tensor], out: Tensor | None = None) -> Tensor: ...
def split(tensor: Tensor, split_size_or_sections: Any, dim: int = 0) -> list[Tensor]: ...
def chunk(input: Tensor, chunks: int, dim: int = 0) -> list[Tensor]: ...
def unbind(input: Tensor, dim: int = 0) -> tuple[Tensor, ...]: ...

def squeeze(input: Tensor, dim: int | None = None) -> Tensor: ...
def unsqueeze(input: Tensor, dim: int) -> Tensor: ...
def transpose(input: Tensor, dim0: int, dim1: int) -> Tensor: ...
def permute(input: Tensor, dims: Any) -> Tensor: ...
def reshape(input: Tensor, shape: Any) -> Tensor: ...
def flatten(input: Tensor, start_dim: int = 0, end_dim: int = -1) -> Tensor: ...
def flip(input: Tensor, dims: Any) -> Tensor: ...
def rot90(input: Tensor, k: int = 1, dims: Any = None) -> Tensor: ...


# --- Math operations ---
def abs(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def absolute(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def add(input: Tensor, other: Any, alpha: Any = 1, out: Tensor | None = None) -> Tensor: ...
def sub(input: Tensor, other: Any, alpha: Any = 1, out: Tensor | None = None) -> Tensor: ...
def subtract(input: Tensor, other: Any, alpha: Any = 1, out: Tensor | None = None) -> Tensor: ...
def mul(input: Tensor, other: Any, out: Tensor | None = None) -> Tensor: ...
def multiply(input: Tensor, other: Any, out: Tensor | None = None) -> Tensor: ...
def div(input: Tensor, other: Any, rounding_mode: str | None = None, out: Tensor | None = None) -> Tensor: ...
def divide(input: Tensor, other: Any, rounding_mode: str | None = None, out: Tensor | None = None) -> Tensor: ...
def pow(input: Tensor, exponent: Any, out: Tensor | None = None) -> Tensor: ...
def sqrt(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def rsqrt(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def square(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def exp(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def expm1(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def log(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def log2(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def log10(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def log1p(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def neg(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def negative(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def reciprocal(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def sign(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def floor(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def ceil(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def round(input: Tensor, decimals: int = 0, out: Tensor | None = None) -> Tensor: ...
def trunc(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def frac(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def clamp(input: Tensor, min: Any | None = None, max: Any | None = None, out: Tensor | None = None) -> Tensor: ...
def clip(input: Tensor, min: Any | None = None, max: Any | None = None, out: Tensor | None = None) -> Tensor: ...

def sin(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def cos(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def tan(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def asin(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def acos(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def atan(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def atan2(input: Tensor, other: Tensor, out: Tensor | None = None) -> Tensor: ...
def sinh(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def cosh(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def tanh(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def sigmoid(input: Tensor, out: Tensor | None = None) -> Tensor: ...


# --- Reduction operations ---
def sum(input: Tensor, dim: Any | None = None, keepdim: bool = False, dtype: dtype | None = None, out: Tensor | None = None) -> Tensor: ...
def mean(input: Tensor, dim: Any | None = None, keepdim: bool = False, dtype: dtype | None = None, out: Tensor | None = None) -> Tensor: ...
def std(input: Tensor, dim: Any | None = None, unbiased: bool = True, keepdim: bool = False, out: Tensor | None = None) -> Tensor: ...
def var(input: Tensor, dim: Any | None = None, unbiased: bool = True, keepdim: bool = False, out: Tensor | None = None) -> Tensor: ...
def prod(input: Tensor, dim: int | None = None, keepdim: bool = False, dtype: dtype | None = None) -> Tensor: ...
def max(input: Tensor, dim: int | None = None, keepdim: bool = False, out: Any | None = None) -> Any: ...
def min(input: Tensor, dim: int | None = None, keepdim: bool = False, out: Any | None = None) -> Any: ...
def amax(input: Tensor, dim: Any | None = None, keepdim: bool = False, out: Tensor | None = None) -> Tensor: ...
def amin(input: Tensor, dim: Any | None = None, keepdim: bool = False, out: Tensor | None = None) -> Tensor: ...
def argmax(input: Tensor, dim: int | None = None, keepdim: bool = False) -> Tensor: ...
def argmin(input: Tensor, dim: int | None = None, keepdim: bool = False) -> Tensor: ...
def all(input: Tensor, dim: int | None = None, keepdim: bool = False, out: Tensor | None = None) -> Tensor: ...
def any(input: Tensor, dim: int | None = None, keepdim: bool = False, out: Tensor | None = None) -> Tensor: ...
def norm(input: Tensor, p: Any = "fro", dim: Any | None = None, keepdim: bool = False, dtype: dtype | None = None, out: Tensor | None = None) -> Tensor: ...
def trace(input: Tensor) -> Tensor: ...


# --- Comparison operations ---
def eq(input: Tensor, other: Any, out: Tensor | None = None) -> Tensor: ...
def ne(input: Tensor, other: Any, out: Tensor | None = None) -> Tensor: ...
def gt(input: Tensor, other: Any, out: Tensor | None = None) -> Tensor: ...
def ge(input: Tensor, other: Any, out: Tensor | None = None) -> Tensor: ...
def lt(input: Tensor, other: Any, out: Tensor | None = None) -> Tensor: ...
def le(input: Tensor, other: Any, out: Tensor | None = None) -> Tensor: ...
def equal(input: Tensor, other: Tensor) -> bool: ...
def allclose(input: Tensor, other: Tensor, rtol: float = 1e-05, atol: float = 1e-08, equal_nan: bool = False) -> bool: ...
def isclose(input: Tensor, other: Tensor, rtol: float = 1e-05, atol: float = 1e-08, equal_nan: bool = False) -> Tensor: ...
def isnan(input: Tensor) -> Tensor: ...
def isinf(input: Tensor) -> Tensor: ...
def isfinite(input: Tensor) -> Tensor: ...
def isreal(input: Tensor) -> Tensor: ...


# --- Sorting and selection ---
def topk(input: Tensor, k: int, dim: int = -1, largest: bool = True, sorted: bool = True, out: Any | None = None) -> tuple[Tensor, Tensor]: ...
def sort(input: Tensor, dim: int = -1, descending: bool = False, stable: bool = False, out: Any | None = None) -> tuple[Tensor, Tensor]: ...
def argsort(input: Tensor, dim: int = -1, descending: bool = False, stable: bool = False) -> Tensor: ...
def kthvalue(input: Tensor, k: int, dim: int = -1, keepdim: bool = False, out: Any | None = None) -> tuple[Tensor, Tensor]: ...
def unique(input: Tensor, sorted: bool = True, return_inverse: bool = False, return_counts: bool = False, dim: int | None = None) -> Any: ...


# --- Linear algebra ---
def matmul(input: Tensor, other: Tensor, out: Tensor | None = None) -> Tensor: ...
def mm(input: Tensor, mat2: Tensor, out: Tensor | None = None) -> Tensor: ...
def bmm(input: Tensor, mat2: Tensor, out: Tensor | None = None) -> Tensor: ...
def mv(input: Tensor, vec: Tensor, out: Tensor | None = None) -> Tensor: ...
def dot(input: Tensor, other: Tensor, out: Tensor | None = None) -> Tensor: ...
def inner(input: Tensor, other: Tensor) -> Tensor: ...
def outer(input: Tensor, vec2: Tensor, out: Tensor | None = None) -> Tensor: ...
def einsum(equation: str, *operands: Tensor) -> Tensor: ...
def tensordot(a: Tensor, b: Tensor, dims: Any = 2, out: Tensor | None = None) -> Tensor: ...

def inverse(input: Tensor, out: Tensor | None = None) -> Tensor: ...
def det(input: Tensor) -> Tensor: ...
def logdet(input: Tensor) -> Tensor: ...
def slogdet(input: Tensor) -> tuple[Tensor, Tensor]: ...
def svd(input: Tensor, some: bool = True, compute_uv: bool = True, out: Any | None = None) -> tuple[Tensor, Tensor, Tensor]: ...
def eig(input: Tensor, eigenvectors: bool = False, out: Any | None = None) -> tuple[Tensor, Tensor]: ...
def qr(input: Tensor, some: bool = True, out: Any | None = None) -> tuple[Tensor, Tensor]: ...
def cholesky(input: Tensor, upper: bool = False, out: Tensor | None = None) -> Tensor: ...


# --- Complex operations ---
def complex(real: Tensor, imag: Tensor) -> Tensor: ...
def polar(abs: Tensor, angle: Tensor, out: Tensor | None = None) -> Tensor: ...
def angle(input: Tensor) -> Tensor: ...
def real(input: Tensor) -> Tensor: ...
def imag(input: Tensor) -> Tensor: ...
def conj(input: Tensor) -> Tensor: ...
def conj_physical(input: Tensor, out: Tensor | None = None) -> Tensor: ...


# --- Other operations ---
def where(condition: Tensor, input: Any = None, other: Any = None) -> Tensor: ...
def masked_select(input: Tensor, mask: Tensor, out: Tensor | None = None) -> Tensor: ...
def gather(input: Tensor, dim: int, index: Tensor, sparse_grad: bool = False, out: Tensor | None = None) -> Tensor: ...
def scatter(input: Tensor, dim: int, index: Tensor, src: Any) -> Tensor: ...
def index_select(input: Tensor, dim: int, index: Tensor, out: Tensor | None = None) -> Tensor: ...
def take(input: Tensor, index: Tensor) -> Tensor: ...
def narrow(input: Tensor, dim: int, start: int, length: int) -> Tensor: ...
def softmax(input: Tensor, dim: int, dtype: dtype | None = None) -> Tensor: ...
def log_softmax(input: Tensor, dim: int, dtype: dtype | None = None) -> Tensor: ...
def ldexp(input: Tensor, other: Tensor, out: Tensor | None = None) -> Tensor: ...


# --- Context managers ---
class no_grad:
    def __enter__(self) -> None: ...
    def __exit__(self, *args: Any) -> None: ...
    def __call__(self, func: Any) -> Any: ...

class inference_mode:
    def __init__(self, mode: bool = True) -> None: ...
    def __enter__(self) -> None: ...
    def __exit__(self, *args: Any) -> None: ...
    def __call__(self, func: Any) -> Any: ...

class enable_grad:
    def __enter__(self) -> None: ...
    def __exit__(self, *args: Any) -> None: ...
    def __call__(self, func: Any) -> Any: ...

def set_grad_enabled(mode: bool) -> Any: ...
def is_grad_enabled() -> bool: ...


# --- CUDA ---
class cuda:
    @staticmethod
    def is_available() -> bool: ...
    @staticmethod
    def device_count() -> int: ...
    @staticmethod
    def current_device() -> int: ...
    @staticmethod
    def set_device(device: Any) -> None: ...
    @staticmethod
    def synchronize(device: Any | None = None) -> None: ...
    @staticmethod
    def empty_cache() -> None: ...
    @staticmethod
    def memory_allocated(device: Any | None = None) -> int: ...
    @staticmethod
    def max_memory_allocated(device: Any | None = None) -> int: ...


# --- Distributed ---
class distributed:
    @staticmethod
    def is_available() -> bool: ...
    @staticmethod
    def is_initialized() -> bool: ...
    @staticmethod
    def is_initialised() -> bool: ...
    @staticmethod
    def init_process_group(*args: Any, **kwargs: Any) -> None: ...
    @staticmethod
    def destroy_process_group() -> None: ...
    @staticmethod
    def get_rank() -> int: ...
    @staticmethod
    def get_world_size() -> int: ...


# --- linalg submodule ---
class linalg:
    @staticmethod
    def qr(A: Tensor, mode: str = "reduced") -> tuple[Tensor, Tensor]: ...
    @staticmethod
    def norm(A: Tensor, ord: Any | None = None, dim: Any | None = None, keepdim: bool = False, dtype: dtype | None = None, out: Tensor | None = None) -> Tensor: ...
    @staticmethod
    def eigvals(A: Tensor) -> Tensor: ...
    @staticmethod
    def eig(A: Tensor) -> tuple[Tensor, Tensor]: ...
    @staticmethod
    def eigvalsh(A: Tensor, UPLO: str = "L") -> Tensor: ...
    @staticmethod
    def eigh(A: Tensor, UPLO: str = "L") -> tuple[Tensor, Tensor]: ...
    @staticmethod
    def svd(A: Tensor, full_matrices: bool = True) -> tuple[Tensor, Tensor, Tensor]: ...
    @staticmethod
    def svdvals(A: Tensor) -> Tensor: ...
    @staticmethod
    def inv(A: Tensor) -> Tensor: ...
    @staticmethod
    def pinv(A: Tensor, rcond: float = 1e-15, hermitian: bool = False) -> Tensor: ...
    @staticmethod
    def det(A: Tensor) -> Tensor: ...
    @staticmethod
    def slogdet(A: Tensor) -> tuple[Tensor, Tensor]: ...
    @staticmethod
    def matrix_rank(A: Tensor, tol: float | None = None, hermitian: bool = False) -> Tensor: ...
    @staticmethod
    def cholesky(A: Tensor, upper: bool = False) -> Tensor: ...
    @staticmethod
    def solve(A: Tensor, B: Tensor, left: bool = True) -> Tensor: ...
    @staticmethod
    def lstsq(A: Tensor, B: Tensor, rcond: float | None = None, driver: str | None = None) -> Any: ...


# --- nn module ---
class nn:
    class Module:
        training: bool
        def __init__(self) -> None: ...
        def __call__(self, *args: Any, **kwargs: Any) -> Any: ...
        def forward(self, *args: Any, **kwargs: Any) -> Any: ...
        def to(self, *args: Any, **kwargs: Any) -> nn.Module: ...
        def cpu(self) -> nn.Module: ...
        def cuda(self, device: Any | None = None) -> nn.Module: ...
        def eval(self) -> nn.Module: ...
        def train(self, mode: bool = True) -> nn.Module: ...
        def parameters(self, recurse: bool = True) -> Iterator[Tensor]: ...
        def named_parameters(self, prefix: str = "", recurse: bool = True) -> Iterator[tuple[str, Tensor]]: ...
        def buffers(self, recurse: bool = True) -> Iterator[Tensor]: ...
        def named_buffers(self, prefix: str = "", recurse: bool = True) -> Iterator[tuple[str, Tensor]]: ...
        def children(self) -> Iterator[nn.Module]: ...
        def named_children(self) -> Iterator[tuple[str, nn.Module]]: ...
        def modules(self) -> Iterator[nn.Module]: ...
        def named_modules(self, memo: Any | None = None, prefix: str = "", remove_duplicate: bool = True) -> Iterator[tuple[str, nn.Module]]: ...
        def state_dict(self, destination: Any | None = None, prefix: str = "", keep_vars: bool = False) -> dict[str, Any]: ...
        def load_state_dict(self, state_dict: dict[str, Any], strict: bool = True) -> Any: ...
        def register_buffer(self, name: str, tensor: Tensor | None, persistent: bool = True) -> None: ...
        def register_parameter(self, name: str, param: nn.Parameter | None) -> None: ...
        def apply(self, fn: Any) -> nn.Module: ...
        def zero_grad(self, set_to_none: bool = False) -> None: ...
        def __getattr__(self, name: str) -> Any: ...
        def __setattr__(self, name: str, value: Any) -> None: ...

    class Parameter(Tensor):
        def __init__(self, data: Tensor = ..., requires_grad: bool = True) -> None: ...

    class ModuleList:
        def __init__(self, modules: Any | None = None) -> None: ...
        def __getitem__(self, idx: int) -> nn.Module: ...
        def __setitem__(self, idx: int, module: nn.Module) -> None: ...
        def __len__(self) -> int: ...
        def __iter__(self) -> Iterator[nn.Module]: ...
        def append(self, module: nn.Module) -> nn.ModuleList: ...
        def extend(self, modules: Any) -> nn.ModuleList: ...
        def insert(self, index: int, module: nn.Module) -> None: ...

    class ModuleDict:
        def __init__(self, modules: Any | None = None) -> None: ...
        def __getitem__(self, key: str) -> nn.Module: ...
        def __setitem__(self, key: str, module: nn.Module) -> None: ...
        def __len__(self) -> int: ...
        def __iter__(self) -> Iterator[str]: ...
        def keys(self) -> Any: ...
        def values(self) -> Any: ...
        def items(self) -> Any: ...
        def update(self, modules: Any) -> None: ...

    class Linear(Module):
        weight: Tensor
        bias: Tensor | None
        in_features: int
        out_features: int
        def __init__(self, in_features: int, out_features: int, bias: bool = True, device: Any | None = None, dtype: dtype | None = None) -> None: ...

    class Embedding(Module):
        weight: Tensor
        num_embeddings: int
        embedding_dim: int
        padding_idx: int | None
        def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int | None = None, max_norm: float | None = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, sparse: bool = False, _weight: Tensor | None = None, device: Any | None = None, dtype: dtype | None = None) -> None: ...

    class LayerNorm(Module):
        weight: Tensor
        bias: Tensor
        normalized_shape: tuple[int, ...]
        eps: float
        def __init__(self, normalized_shape: Any, eps: float = 1e-5, elementwise_affine: bool = True, device: Any | None = None, dtype: dtype | None = None) -> None: ...

    class RMSNorm(Module):
        weight: Tensor
        eps: float
        def __init__(self, normalized_shape: Any, eps: float = 1e-5, elementwise_affine: bool = True, device: Any | None = None, dtype: dtype | None = None) -> None: ...

    class BatchNorm1d(Module):
        weight: Tensor
        bias: Tensor
        running_mean: Tensor
        running_var: Tensor
        def __init__(self, num_features: int, eps: float = 1e-5, momentum: float = 0.1, affine: bool = True, track_running_stats: bool = True, device: Any | None = None, dtype: dtype | None = None) -> None: ...

    class BatchNorm2d(Module):
        weight: Tensor
        bias: Tensor
        running_mean: Tensor
        running_var: Tensor
        def __init__(self, num_features: int, eps: float = 1e-5, momentum: float = 0.1, affine: bool = True, track_running_stats: bool = True, device: Any | None = None, dtype: dtype | None = None) -> None: ...

    class Dropout(Module):
        p: float
        inplace: bool
        def __init__(self, p: float = 0.5, inplace: bool = False) -> None: ...

    class Conv1d(Module):
        weight: Tensor
        bias: Tensor | None
        def __init__(self, in_channels: int, out_channels: int, kernel_size: Any, stride: Any = 1, padding: Any = 0, dilation: Any = 1, groups: int = 1, bias: bool = True, padding_mode: str = "zeros", device: Any | None = None, dtype: dtype | None = None) -> None: ...

    class Conv2d(Module):
        weight: Tensor
        bias: Tensor | None
        def __init__(self, in_channels: int, out_channels: int, kernel_size: Any, stride: Any = 1, padding: Any = 0, dilation: Any = 1, groups: int = 1, bias: bool = True, padding_mode: str = "zeros", device: Any | None = None, dtype: dtype | None = None) -> None: ...

    class ReLU(Module):
        inplace: bool
        def __init__(self, inplace: bool = False) -> None: ...

    class GELU(Module):
        approximate: str
        def __init__(self, approximate: str = "none") -> None: ...

    class SiLU(Module):
        inplace: bool
        def __init__(self, inplace: bool = False) -> None: ...

    class Sigmoid(Module): ...
    class Tanh(Module): ...
    class Softmax(Module):
        dim: int
        def __init__(self, dim: int) -> None: ...

    class CrossEntropyLoss(Module):
        def __init__(self, weight: Tensor | None = None, size_average: bool | None = None, ignore_index: int = -100, reduce: bool | None = None, reduction: str = "mean", label_smoothing: float = 0.0) -> None: ...

    class MSELoss(Module):
        def __init__(self, size_average: bool | None = None, reduce: bool | None = None, reduction: str = "mean") -> None: ...

    class BCELoss(Module):
        def __init__(self, weight: Tensor | None = None, size_average: bool | None = None, reduce: bool | None = None, reduction: str = "mean") -> None: ...

    class BCEWithLogitsLoss(Module):
        def __init__(self, weight: Tensor | None = None, size_average: bool | None = None, reduce: bool | None = None, reduction: str = "mean", pos_weight: Tensor | None = None) -> None: ...

    class functional:
        @staticmethod
        def relu(input: Tensor, inplace: bool = False) -> Tensor: ...
        @staticmethod
        def gelu(input: Tensor, approximate: str = "none") -> Tensor: ...
        @staticmethod
        def silu(input: Tensor, inplace: bool = False) -> Tensor: ...
        @staticmethod
        def sigmoid(input: Tensor) -> Tensor: ...
        @staticmethod
        def tanh(input: Tensor) -> Tensor: ...
        @staticmethod
        def softmax(input: Tensor, dim: int, dtype: dtype | None = None) -> Tensor: ...
        @staticmethod
        def log_softmax(input: Tensor, dim: int, dtype: dtype | None = None) -> Tensor: ...
        @staticmethod
        def dropout(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor: ...
        @staticmethod
        def linear(input: Tensor, weight: Tensor, bias: Tensor | None = None) -> Tensor: ...
        @staticmethod
        def embedding(input: Tensor, weight: Tensor, padding_idx: int | None = None, max_norm: float | None = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, sparse: bool = False) -> Tensor: ...
        @staticmethod
        def layer_norm(input: Tensor, normalized_shape: Any, weight: Tensor | None = None, bias: Tensor | None = None, eps: float = 1e-5) -> Tensor: ...
        @staticmethod
        def normalize(input: Tensor, p: float = 2.0, dim: int = 1, eps: float = 1e-12, out: Tensor | None = None) -> Tensor: ...
        @staticmethod
        def cosine_similarity(x1: Tensor, x2: Tensor, dim: int = 1, eps: float = 1e-8) -> Tensor: ...
        @staticmethod
        def cross_entropy(input: Tensor, target: Tensor, weight: Tensor | None = None, size_average: bool | None = None, ignore_index: int = -100, reduce: bool | None = None, reduction: str = "mean", label_smoothing: float = 0.0) -> Tensor: ...
        @staticmethod
        def mse_loss(input: Tensor, target: Tensor, size_average: bool | None = None, reduce: bool | None = None, reduction: str = "mean") -> Tensor: ...
        @staticmethod
        def binary_cross_entropy(input: Tensor, target: Tensor, weight: Tensor | None = None, size_average: bool | None = None, reduce: bool | None = None, reduction: str = "mean") -> Tensor: ...
        @staticmethod
        def binary_cross_entropy_with_logits(input: Tensor, target: Tensor, weight: Tensor | None = None, size_average: bool | None = None, reduce: bool | None = None, reduction: str = "mean", pos_weight: Tensor | None = None) -> Tensor: ...
        @staticmethod
        def conv1d(input: Tensor, weight: Tensor, bias: Tensor | None = None, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1) -> Tensor: ...
        @staticmethod
        def conv2d(input: Tensor, weight: Tensor, bias: Tensor | None = None, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1) -> Tensor: ...
        @staticmethod
        def interpolate(input: Tensor, size: Any | None = None, scale_factor: Any | None = None, mode: str = "nearest", align_corners: bool | None = None, recompute_scale_factor: bool | None = None, antialias: bool = False) -> Tensor: ...
        @staticmethod
        def pad(input: Tensor, pad: Any, mode: str = "constant", value: float = 0.0) -> Tensor: ...
        @staticmethod
        def one_hot(tensor: Tensor, num_classes: int = -1) -> Tensor: ...
        @staticmethod
        def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor, attn_mask: Tensor | None = None, dropout_p: float = 0.0, is_causal: bool = False, scale: float | None = None) -> Tensor: ...
