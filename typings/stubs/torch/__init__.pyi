# Minimal PyTorch type stubs for essential functions
from typing import Any, Optional, Union, Tuple, Sequence

class dtype:
    def __init__(self) -> None: ...

class device:
    def __init__(self, device: Union[str, int]) -> None: ...

class Tensor:
    def __init__(self) -> None: ...
    @property
    def dtype(self) -> dtype: ...
    @property
    def shape(self) -> Tuple[int, ...]: ...
    @property
    def T(self) -> "Tensor": ...
    @property
    def real(self) -> "Tensor": ...
    @property
    def imag(self) -> "Tensor": ...
    def size(self, dim: Optional[int] = None) -> Union[int, Tuple[int, ...]]: ...
    def dim(self) -> int: ...
    def view(self, *shape: int) -> "Tensor": ...
    def to(self, dtype_or_device: Union[dtype, device, str], **kwargs: Any) -> "Tensor": ...
    def contiguous(self) -> "Tensor": ...
    def reshape(self, *shape: int) -> "Tensor": ...
    def unsqueeze(self, dim: int) -> "Tensor": ...
    def squeeze(self, dim: Optional[int] = None) -> "Tensor": ...
    def float(self) -> "Tensor": ...
    def element_size(self) -> int: ...
    def repeat_interleave(self, repeats: Union[int, "Tensor"], dim: Optional[int] = None) -> "Tensor": ...
    def item(self) -> Union[int, float]: ...
    def clone(self) -> "Tensor": ...
    def cpu(self) -> "Tensor": ...
    def cuda(self, device: Optional[Union[device, int, str]] = None) -> "Tensor": ...
    def numpy(self) -> Any: ...
    def tolist(self) -> Any: ...
    def pow(self, exponent: Union[int, float, "Tensor"]) -> "Tensor": ...
    def mean(self, dim: Optional[Union[int, Tuple[int, ...]]] = None, keepdim: bool = False) -> "Tensor": ...
    def sum(self, dim: Optional[Union[int, Tuple[int, ...]]] = None, keepdim: bool = False) -> "Tensor": ...
    def std(self, dim: Optional[int] = None, keepdim: bool = False, unbiased: bool = True) -> "Tensor": ...
    def var(self, dim: Optional[Union[int, Tuple[int, ...]]] = None, keepdim: bool = False, unbiased: bool = True) -> "Tensor": ...
    def norm(self, p: Union[int, float, str] = 2, dim: Optional[Union[int, Tuple[int, ...]]] = None, keepdim: bool = False) -> "Tensor": ...
    def flatten(self, start_dim: int = 0, end_dim: int = -1) -> "Tensor": ...
    def detach(self) -> "Tensor": ...
    def __iter__(self) -> Any: ...
    def __len__(self) -> int: ...
    def __getitem__(self, key: Any) -> "Tensor": ...
    def __setitem__(self, key: Any, value: Any) -> None: ...
    def __xor__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __matmul__(self, other: "Tensor") -> "Tensor": ...
    def __mul__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __rmul__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __add__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __radd__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __sub__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __rsub__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __truediv__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __rtruediv__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __pow__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __rpow__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __neg__(self) -> "Tensor": ...
    def __gt__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __lt__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __ge__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __le__(self, other: Union["Tensor", int, float]) -> "Tensor": ...
    def __float__(self) -> float: ...

# Data types
uint8: dtype
int16: dtype
float16: dtype
bfloat16: dtype
float32: dtype
int32: dtype
int64: dtype
bool_dtype: dtype
bool: dtype  # torch.bool

# Core functions
def tensor(data: Any, dtype: Optional[dtype] = None, device: Optional[Union[device, str]] = None) -> Tensor: ...
def zeros(*size: int, dtype: Optional[dtype] = None, device: Optional[Union[device, str]] = None) -> Tensor: ...
def full(
    size: Tuple[int, ...],
    fill_value: Union[int, float],
    dtype: Optional[dtype] = None,
    device: Optional[Union[device, str]] = None,
) -> Tensor: ...
def arange(
    start: Union[int, float],
    end: Optional[Union[int, float]] = None,
    step: Union[int, float] = 1,
    dtype: Optional[dtype] = None,
    device: Optional[Union[device, str]] = None,
) -> Tensor: ...
def cat(tensors: Sequence[Tensor], dim: int = 0) -> Tensor: ...
def stack(tensors: Sequence[Tensor], dim: int = 0) -> Tensor: ...
def topk(
    input: Tensor, k: int, dim: Optional[int] = None, largest: bool = True, sorted: bool = True
) -> Tuple[Tensor, Tensor]: ...
def cos(input: Tensor) -> Tensor: ...
def sin(input: Tensor) -> Tensor: ...
def isnan(input: Tensor) -> Tensor: ...
def isinf(input: Tensor) -> Tensor: ...
def ldexp(input: Tensor, other: Tensor, *, out: Optional[Tensor] = None) -> Tensor: ...
def rsqrt(input: Tensor) -> Tensor: ...
def norm(input: Tensor, p: Union[int, float, str] = 2, dim: Optional[int] = None, keepdim: bool = False) -> Tensor: ...
def randn(*size: int, dtype: Optional[dtype] = None, device: Optional[Union[device, str]] = None) -> Tensor: ...
def argmax(input: Tensor, dim: Optional[int] = None, keepdim: bool = False) -> Tensor: ...
def from_numpy(ndarray: Any) -> Tensor: ...
def std(input: Tensor, dim: Optional[int] = None, keepdim: bool = False, unbiased: bool = True) -> Tensor: ...
def sum(input: Tensor, dim: Optional[int] = None, keepdim: bool = False) -> Tensor: ...
def log(input: Tensor) -> Tensor: ...
def softmax(input: Tensor, dim: int) -> Tensor: ...
def randint(low: int, high: int, size: Tuple[int, ...], dtype: Optional[dtype] = None, device: Optional[Union[device, str]] = None) -> Tensor: ...
def var(input: Tensor, dim: Optional[Union[int, Tuple[int, ...]]] = None, keepdim: bool = False, unbiased: bool = True) -> Tensor: ...
def mean(input: Tensor, dim: Optional[Union[int, Tuple[int, ...]]] = None, keepdim: bool = False) -> Tensor: ...
def eye(n: int, m: Optional[int] = None, dtype: Optional[dtype] = None, device: Optional[Union[device, str]] = None) -> Tensor: ...
def trace(input: Tensor) -> Tensor: ...
def angle(input: Tensor) -> Tensor: ...
def abs(input: Tensor) -> Tensor: ...
def randperm(n: int, dtype: Optional[dtype] = None, device: Optional[Union[device, str]] = None) -> Tensor: ...
def complex(real: Tensor, imag: Tensor) -> Tensor: ...

# Context managers
class no_grad:
    def __enter__(self) -> None: ...
    def __exit__(self, *args: Any) -> None: ...

# CUDA
class cuda:
    @staticmethod
    def is_available() -> bool: ...

# Distributed module
class distributed:
    @staticmethod
    def is_available() -> bool: ...
    @staticmethod
    def is_initialised() -> bool: ...
    @staticmethod
    def init_process_group(*args: Any, **kwargs: Any) -> None: ...
    @staticmethod
    def destroy_process_group() -> None: ...

# linalg submodule
class linalg:
    @staticmethod
    def qr(A: Tensor, mode: str = "reduced") -> Tuple[Tensor, Tensor]: ...
    @staticmethod
    def norm(A: Tensor, ord: Optional[Union[int, float, str]] = None, dim: Optional[int] = None, keepdim: bool = False) -> Tensor: ...
    @staticmethod
    def eigvals(A: Tensor) -> Tensor: ...
    @staticmethod
    def svdvals(A: Tensor) -> Tensor: ...
    @staticmethod
    def svd(A: Tensor, full_matrices: bool = True) -> Tuple[Tensor, Tensor, Tensor]: ...

# nn submodule
class nn:
    class functional:
        @staticmethod
        def silu(input: Tensor, inplace: bool = False) -> Tensor: ...
        @staticmethod
        def softmax(input: Tensor, dim: int, dtype: Optional[dtype] = None) -> Tensor: ...
        @staticmethod
        def relu(input: Tensor, inplace: bool = False) -> Tensor: ...
        @staticmethod
        def normalize(input: Tensor, p: float = 2.0, dim: int = 1, eps: float = 1e-12) -> Tensor: ...
        @staticmethod
        def cosine_similarity(x1: Tensor, x2: Tensor, dim: int = 1, eps: float = 1e-8) -> Tensor: ...
