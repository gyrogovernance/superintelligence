(.venv) PS F:\Development\superintelligence> python secret_lab_ignore/blomo_port/lab.py Loading Bolmo-1B on cpu (dtype=torch.float32)...
Loading weights: 100%|█| 263/263 [00:00<00:00, 2784.46it/s, Materializing param=model.n
Loaded. Parameters: 1,468,911,776

--- Module 0: Baseline Generation ---
Prompt: 'Language modeling is '
  220 bytes in 13.91s (15.8 tok/s)
  Output: Language modeling is a fundamental task in natural language processing (NLP). It is a key component of machine translation, automatic speech recognition, and a variety of other applications. Traditional methods for language modeling typicall

Prompt: 'The quick brown fox '
  219 bytes in 15.17s (14.4 tok/s)
  Output: The quick brown fox jumps over the lazy dog"\n\nThis pun is said to have been premixed with the joke of Fred Flintstone and Barney Rubble climbing a tree (and then jumping over a lazy dog).\n\nThe phrase "quick brown fox jumping over a lazy do

Prompt: 'def fibonacci(n):\n'
  217 bytes in 18.94s (11.5 tok/s)
  Output: def fibonacci(n):\n    """ Returns a fibonacci number between 1 and n-1 """\n    if n == 1:\n        return 1\n    elif n == 2:\n        return 1\n    elif (n-1) == 2:\n        return 2\n    else:\n        return fibonacci(n-1)+fibonacci(n-2)\n\n        


--- Module 1: Kernel Observer ---
Prompt: 'Language modeling is '
  Trajectory: 220 bytes
  Boundaries: 0 (0.0%)
  Horizon coverage: 144/256
  Vertex dist: {0: 81, 1: 53, 2: 34, 3: 52}
  Phase dist: {0: 53, 1: 47, 2: 69, 3: 51}
  Family dist: {2: 36, 3: 184}
  Mean mask weight: 6.64
  Mean horizon dist: 5.67
  Mean code distance: 4.81
  Phase transition dist: {0: 46, 1: 87, 2: 46, 3: 40}
  Vertex change rate: 0.776
  First 40 bytes:
   pos byte chr mic fam mw   h HD chi ph fsd
     0   97   a  11   3  8 170  8   0  2   .
     1   32      10   2  5  97  3   3  3   .
     2  102   f  12   3  6  32  7   0  0   .
     3  117   u  31   3 11 173  4   3  1   .
     4  110   n   4   3  4 255  6   0  3   .
     5  100   d  14   3  8 105  4   1  3   .
     6   97   a  11   3  8  49  6   0  0   .
     7  109   m   7   3  8 162  4   0  1   .
     8  101   e  15   3 10 246  8   0  1   .
     9  110   n   4   3  4 109 10   0  3   .
    10  116   t  30   3  9  50  3   2  0   .
    11   97   a  11   3  8 179  5   0  1   .
    12  108   l   6   3  6 249  5   1  1   .
    13   32      10   2  5 117  4   3  3   .
    14  116   t  30   3  9 115  5   2  0   .
    15   97   a  11   3  8 171  5   0  2   .
    16  115   s  25   3  7 184  6   2  2   .
    17  107   k   1   3  4 114  6   1  0   .
    18   32      10   2  5 121  3   3  1   .
    19  105   i   3   3  6 248  3   0  0   .
    20  110   n   4   3  4 186  5   0  1   .
    21   32      10   2  5  60  4   3  3   .
    22  110   n   4   3  4  48  4   0  0   .
    23   97   a  11   3  8 248  4   0  1   .
    24  116   t  30   3  9 251  9   2  2   .
    25  117   u  31   3 11  38  2   3  3   .
    26  114   r  24   3  5  36  7   3  0   .
    27   97   a  11   3  8 254  3   0  1   .
    28  108   l   6   3  6 239  9   1  3   .
    29   32      10   2  5  56  8   3  2   .
    30  108   l   6   3  6 101  8   1  2   .
    31   97   a  11   3  8 254  2   0  2   .
    32  110   n   4   3  4 174  4   0  3   .
    33  103   g  13   3  8  58  6   1  3   .
    34  117   u  31   3 11  99  5   3  0   .
    35   97   a  11   3  8 229  7   0  1   .
    36  103   g  13   3  8 168  1   1  2   .
    37  101   e  15   3 10  40  9   0  3   .
    38   32      10   2  5 103  6   3  3   .
    39  112   p  26   3  7 162  9   2  0   .

Prompt: 'The quick brown fox '
  Trajectory: 219 bytes
  Boundaries: 0 (0.0%)
  Horizon coverage: 141/256
  Vertex dist: {0: 72, 1: 49, 2: 28, 3: 70}
  Phase dist: {0: 55, 1: 56, 2: 51, 3: 57}
  Family dist: {2: 48, 3: 171}
  Mean mask weight: 6.34
  Mean horizon dist: 5.93
  Mean code distance: 4.92
  Phase transition dist: {0: 37, 1: 87, 2: 46, 3: 48}
  Vertex change rate: 0.752
  First 40 bytes:
   pos byte chr mic fam mw   h HD chi ph fsd
     0  106   j   0   3  2 170  2   0  2   .
     1  117   u  31   3 11 106  9   3  3   .
     2  109   m   7   3  8 117  5   0  2   .
     3  112   p  26   3  7 173  2   2  1   .
     4  115   s  25   3  7 175  9   2  3   .
     5   32      10   2  5 116  4   3  0   .
     6  111   o   5   3  6  37  4   1  2   .
     7  118   v  28   3  7 177  3   3  0   .
     8  101   e  15   3 10 249  7   0  1   .
     9  114   r  24   3  5 126 10   3  3   .
    10   32      10   2  5  33  7   3  0   .
    11  116   t  30   3  9 244  6   2  1   .
    12  104   h   2   3  4 255  6   1  1   .
    13  101   e  15   3 10  54  4   0  3   .
    14   32      10   2  5  48  5   3  0   .
    15  108   l   6   3  6 188  5   1  1   .
    16   97   a  11   3  8 246  3   0  0   .
    17  122   z  16   3  3 119  4   3  3   .
    18  121   y  19   3  7  38  3   3  3   .
    19   32      10   2  5 164  2   3  0   .
    20  100   d  14   3  8 172  6   1  2   .
    21  111   o   5   3  6 106  4   1  3   .
    22  103   g  13   3  8 105  8   1  2   .
    23   34   "   8   2  3 167  5   2  3   .
    24   10   .  32   2  2 225  7   1  2   .
    25   10   .  32   2  2   7  5   1  3   .
    26   84   T  62   3 10  65  5   1  0   .
    27  104   h   2   3  4 249  7   1  1   .
    28  105   i   3   3  6 131  7   0  2   .
    29  115   s  25   3  7  58  2   2  1   .
    30   32      10   2  5  90  7   3  3   .
    31  112   p  26   3  7 176  2   2  3   .
    32  117   u  31   3 11 128 11   3  0   .
    33  110   n   4   3  4 111  7   0  1   .
    34   32      10   2  5  68  4   3  0   .
    35  105   i   3   3  6 229  4   0  1   .
    36  115   s  25   3  7 135  9   2  1   .
    37   32      10   2  5  60  4   3  2   .
    38  115   s  25   3  7  13  5   2  2   .
    39   97   a  11   3  8 229  5   0  0   .

Prompt: 'def fibonacci(n):\n'
  Trajectory: 217 bytes
  Boundaries: 0 (0.0%)
  Horizon coverage: 125/256
  Vertex dist: {0: 57, 1: 44, 2: 22, 3: 94}
  Phase dist: {0: 52, 1: 54, 2: 57, 3: 54}
  Family dist: {2: 122, 3: 95}
  Mean mask weight: 5.65
  Mean horizon dist: 6.47
  Mean code distance: 3.66
  Phase transition dist: {0: 25, 1: 118, 2: 30, 3: 43}
  Vertex change rate: 0.560
  First 40 bytes:
   pos byte chr mic fam mw   h HD chi ph fsd
     0   32      10   2  5 170  5   3  2   .
     1   32      10   2  5  32  0   3  3   .
     2   32      10   2  5  32  5   3  0   .
     3   32      10   2  5 170  0   3  1   .
     4   34   "   8   2  3 170  3   2  2   .
     5   34   "   8   2  3  34  0   2  3   .
     6   34   "   8   2  3  34  3   2  0   .
     7   32      10   2  5 170  2   3  2   .
     8   82   R  56   3  6 168  8   0  2   .
     9  101   e  15   3 10  82  6   0  0   .
    10  116   t  30   3  9 103  9   2  1   .
    11  117   u  31   3 11 140  4   3  2   .
    12  114   r  24   3  5 184  7   3  2   .
    13  110   n   4   3  4  84  3   0  2   .
    14  115   s  25   3  7 124  6   2  1   .
    15   32      10   2  5 141  9   3  3   .
    16   97   a  11   3  8 246  3   0  3   .
    17   32      10   2  5  70  6   3  0   .
    18  102   f  12   3  6 124  8   0  2   .
    19  105   i   3   3  6 138  6   0  2   .
    20   98   b   8   3  4 191 10   0  3   .
    21  111   o   5   3  6  66  4   1  0   .
    22  110   n   4   3  4 122  8   0  1   .
    23   97   a  11   3  8 134  8   0  3   .
    24   99   c   9   3  6 177 10   1  0   .
    25   99   c   9   3  6  79  8   1  1   .
    26  105   i   3   3  6 120  6   0  0   .
    27   32      10   2  5 140  9   3  3   .
    28  110   n   4   3  4 242  7   0  0   .
    29  117   u  31   3 11  72  6   3  1   .
    30  109   m   7   3  8  45  4   0  1   .
    31   98   b   8   3  4 143  6   0  3   .
    32  101   e  15   3 10 229  6   0  3   .
    33  114   r  24   3  5  64  9   3  0   .
    34   32      10   2  5  61 10   3  1   .
    35   98   b   8   3  4 202 10   0  2   .
    36  101   e  15   3 10 245  4   0  3   .
    37  116   t  30   3  9   5  7   2  0   .
    38  119   w  29   3  9  43  8   2  1   .
    39  101   e  15   3 10 216  6   0  3   .


Done.
(.venv) PS F:\Development\superintelligence> python secret_lab_ignore/blomo_port/lab.py --module 2
Loading Bolmo-1B on cpu (dtype=torch.float32)...
Loading weights: 100%|█| 263/263 [00:00<00:00, 2901.21it/s, Materializing param=model.n
Loaded. Parameters: 1,468,911,776

--- Module 2: Build boundary_adaptor.npz ---
Loaded boundary scores from cache (F:\Development\superintelligence\data\cache\blomo_port\69eb4607b69f0909)
Bolmo boundary score statistics:
  mean: 0.9148
  std: 0.2476
  min: 0.0005
  max: 1.0000
  median: 0.9994
  q25: 0.9974
  q75: 0.9998
  rate_gt_0p5: 0.9137

Exported boundary_adaptor.npz -> F:\Development\superintelligence\data\cache\blomo_port\analysis\boundary_adaptor.npz
frac_residual: 0.573042
  K=2048: residual R²=0.554306  full-logit R²=0.744598
  K=4096: residual R²=0.643910  full-logit R²=0.795945
  K=8192: residual R²=0.748704  full-logit R²=0.855997
  K=16384: residual R²=0.865966  full-logit R²=0.923193
  K=32768: residual R²=0.967106  full-logit R²=0.981150

Done.
(.venv) PS F:\Development\superintelligence> python secret_lab_ignore/blomo_port/lab.py --module 3
Loading Bolmo-1B on cpu (dtype=torch.float32)...
Loading weights: 100%|█| 263/263 [00:00<00:00, 3162.74it/s, Materializing param=model.n
Loaded. Parameters: 1,468,911,776

--- Module 3: Prefill boundary eval (adaptor vs Bolmo) ---

K=2048:
  prompt='Language modeling is '
    compared=20/22  R2=0.8979  pearson=0.9616  MAE=0.0576  agree@0.5=1.000
  prompt='The quick brown fox '
    compared=19/21  R2=0.9028  pearson=0.9647  MAE=0.0762  agree@0.5=1.000
  prompt='def fibonacci(n):\n'
    compared=17/19  R2=0.7217  pearson=0.8883  MAE=0.0937  agree@0.5=0.941
  prompt='Hello world! This is a test of boundaries.\n'
    compared=42/44  R2=0.9083  pearson=0.9625  MAE=0.0550  agree@0.5=0.976

K=8192:
  prompt='Language modeling is '
    compared=20/22  R2=0.9689  pearson=0.9888  MAE=0.0331  agree@0.5=1.000
  prompt='The quick brown fox '
    compared=19/21  R2=0.9802  pearson=0.9925  MAE=0.0357  agree@0.5=1.000
  prompt='def fibonacci(n):\n'
    compared=17/19  R2=0.9321  pearson=0.9691  MAE=0.0501  agree@0.5=1.000
  prompt='Hello world! This is a test of boundaries.\n'
    compared=42/44  R2=0.9400  pearson=0.9756  MAE=0.0445  agree@0.5=1.000

K=16384:
  prompt='Language modeling is '
    compared=20/22  R2=0.9825  pearson=0.9936  MAE=0.0243  agree@0.5=1.000
  prompt='The quick brown fox '
    compared=19/21  R2=0.9748  pearson=0.9888  MAE=0.0305  agree@0.5=1.000
  prompt='def fibonacci(n):\n'
    compared=17/19  R2=0.9654  pearson=0.9829  MAE=0.0343  agree@0.5=1.000
  prompt='Hello world! This is a test of boundaries.\n'
    compared=42/44  R2=0.9481  pearson=0.9781  MAE=0.0385  agree@0.5=1.000

K=32768:
  prompt='Language modeling is '
    compared=20/22  R2=0.9873  pearson=0.9955  MAE=0.0212  agree@0.5=1.000
  prompt='The quick brown fox '
    compared=19/21  R2=0.9897  pearson=0.9955  MAE=0.0206  agree@0.5=1.000
  prompt='def fibonacci(n):\n'
    compared=17/19  R2=0.9660  pearson=0.9834  MAE=0.0352  agree@0.5=1.000
  prompt='Hello world! This is a test of boundaries.\n'
    compared=42/44  R2=0.9787  pearson=0.9910  MAE=0.0234  agree@0.5=1.000

Done.
(.venv) PS F:\Development\superintelligence> python secret_lab_ignore/blomo_port/lab.py --module 4
Loading Bolmo-1B on cpu (dtype=torch.float32)...
Loading weights: 100%|█| 263/263 [00:00<00:00, 2914.87it/s, Materializing param=model.n
Loaded. Parameters: 1,468,911,776

--- Module 4: Patch stats (Bolmo vs Adaptor) ---
Using K=16384, threshold=0.5

prompt='Language modeling is '
  bolmo:   PatchStats(n_bytes=21, n_pairs=20, boundary_rate=0.15, mean_bytes_per_patch=5.25, median_bytes_per_patch=5.5, max_patch=9)
  adaptor: PatchStats(n_bytes=21, n_pairs=20, boundary_rate=0.15, mean_bytes_per_patch=5.25, median_bytes_per_patch=5.5, max_patch=9)

prompt='The quick brown fox '
  bolmo:   PatchStats(n_bytes=20, n_pairs=19, boundary_rate=0.21052631578947367, mean_bytes_per_patch=4.0, median_bytes_per_patch=4.0, max_patch=6)
  adaptor: PatchStats(n_bytes=20, n_pairs=19, boundary_rate=0.21052631578947367, mean_bytes_per_patch=4.0, median_bytes_per_patch=4.0, max_patch=6)

prompt='def fibonacci(n):\n'
  bolmo:   PatchStats(n_bytes=18, n_pairs=17, boundary_rate=0.17647058823529413, mean_bytes_per_patch=4.5, median_bytes_per_patch=3.0, max_patch=10)
  adaptor: PatchStats(n_bytes=18, n_pairs=17, boundary_rate=0.17647058823529413, mean_bytes_per_patch=4.5, median_bytes_per_patch=3.0, max_patch=10)

Done.
(.venv) PS F:\Development\superintelligence> python secret_lab_ignore/blomo_port/lab.py --module 5
Loading Bolmo-1B on cpu (dtype=torch.float32)...
Loading weights: 100%|█| 263/263 [00:00<00:00, 2923.91it/s, Materializing param=model.n
Loaded. Parameters: 1,468,911,776

--- Module 5: Porting suite (analysis + 512 token A/B + deterministic equivalence) ---  

[Module 5] Using K=16384, max_new_tokens=512
Adaptor key: '69eb4607b69f0909'

=== Prefill analysis: 'Language modeling is ' ===
  compared=20/22  R2=0.9825  pearson=0.9936  MAE=0.0243  agree@0.5=1.000
  expected sample mismatches ≈ 0.49 (2.43% per position)
  Patch stats (deterministic threshold=0.5):
    bolmo:   PatchStats(n_bytes=21, n_pairs=20, boundary_rate=0.15, mean_bytes_per_patch=5.25, median_bytes_per_patch=5.5, max_patch=9)
    adaptor: PatchStats(n_bytes=21, n_pairs=20, boundary_rate=0.15, mean_bytes_per_patch=5.25, median_bytes_per_patch=5.5, max_patch=9)
  Patch stats (sampled with shared U seed=123):
    bolmo:   PatchStats(n_bytes=21, n_pairs=20, boundary_rate=0.15, mean_bytes_per_patch=5.25, median_bytes_per_patch=5.5, max_patch=9)
    adaptor: PatchStats(n_bytes=21, n_pairs=20, boundary_rate=0.15, mean_bytes_per_patch=5.25, median_bytes_per_patch=5.5, max_patch=9)
    mismatch rate: 0.000

=== Prefill analysis: 'The quick brown fox ' ===
  compared=19/21  R2=0.9748  pearson=0.9888  MAE=0.0305  agree@0.5=1.000
  expected sample mismatches ≈ 0.58 (3.05% per position)
  Patch stats (deterministic threshold=0.5):
    bolmo:   PatchStats(n_bytes=20, n_pairs=19, boundary_rate=0.21052631578947367, mean_bytes_per_patch=4.0, median_bytes_per_patch=4.0, max_patch=6)
    adaptor: PatchStats(n_bytes=20, n_pairs=19, boundary_rate=0.21052631578947367, mean_bytes_per_patch=4.0, median_bytes_per_patch=4.0, max_patch=6)
  Patch stats (sampled with shared U seed=123):
    bolmo:   PatchStats(n_bytes=20, n_pairs=19, boundary_rate=0.21052631578947367, mean_bytes_per_patch=4.0, median_bytes_per_patch=4.0, max_patch=6)
    adaptor: PatchStats(n_bytes=20, n_pairs=19, boundary_rate=0.21052631578947367, mean_bytes_per_patch=4.0, median_bytes_per_patch=4.0, max_patch=6)
    mismatch rate: 0.000

=== Prefill analysis: 'def fibonacci(n):\n' ===
  compared=17/19  R2=0.9654  pearson=0.9829  MAE=0.0343  agree@0.5=1.000
  expected sample mismatches ≈ 0.58 (3.43% per position)
  Patch stats (deterministic threshold=0.5):
    bolmo:   PatchStats(n_bytes=18, n_pairs=17, boundary_rate=0.17647058823529413, mean_bytes_per_patch=4.5, median_bytes_per_patch=3.0, max_patch=10)
    adaptor: PatchStats(n_bytes=18, n_pairs=17, boundary_rate=0.17647058823529413, mean_bytes_per_patch=4.5, median_bytes_per_patch=3.0, max_patch=10)
  Patch stats (sampled with shared U seed=123):
    bolmo:   PatchStats(n_bytes=18, n_pairs=17, boundary_rate=0.17647058823529413, mean_bytes_per_patch=4.5, median_bytes_per_patch=3.0, max_patch=10)
    adaptor: PatchStats(n_bytes=18, n_pairs=17, boundary_rate=0.17647058823529413, mean_bytes_per_patch=4.5, median_bytes_per_patch=3.0, max_patch=10)
    mismatch rate: 0.000

=== 512-token quality suite on prompt: 'Language modeling is ' ===

[A] Baseline (Bolmo, sampled boundaries)
  tokens=532 time=33.34s
  preview:
Language modeling is a fundamental task in natural language processing (NLP). It is a key component of machine translation, automatic speech recognition, and a variety of other applications. Traditional methods for language modeling typically require a large amount of labeled data, which is often not easily available. In this paper, we propose a novel method for language modeling that uses multitask learning. Our approach uses multiple tasks to learn a distribution over word sequences. We show empirically that the proposed model can effectively pe

[B] Patched (Adaptor, sampled boundaries) K=16384
  tokens=532 time=34.25s
  preview:
Language modeling is not strictly a machine learning technique, because it does not train a model that uses comparisons between words and concepts. Instead, it uses statistics to infer relationships between parts of speech. For example, the word “generate” is likely to occur in contexts where it refers to creating something. This kind of analysis can be used to predict the meaning of words in contexts that do not include the word that follows.\n\nSpeech is one of the most natural and intuitive ways to communicate, and it has been a popular topic

  A vs B token-id match: 40/532 = 0.075
  first divergence step: 0

[C] Baseline deterministic boundaries (Bolmo mask=p>0.5)
  tokens=532 time=33.22s
  preview:
Language modeling is a fundamental task in natural language processing (NLP). It is a key component of machine translation, automatic speech recognition, and a variety of other applications. Traditional methods for language modeling typically require a large amount of labeled data, which is often not easily available. In this paper, we propose a novel method for language modeling that uses multitask learning. Our approach uses multiple tasks to learn a distribution over word sequences. We show empirically that the proposed model can effectively pe

[D] Patched deterministic boundaries (Adaptor mask=p>0.5) K=16384
  tokens=532 time=33.88s
  preview:
Language modeling is a fundamental task in natural language processing (NLP). It is a key component of machine translation, automatic speech recognition, and a variety of other applications. Traditional methods for language modeling typically require a large amount of labeled data, which is often not easily available. In this paper, we propose a novel method for language modeling that uses multitask learning. Our approach uses multiple tasks to learn a distribution over word sequences. We show empirically that the proposed model can effectively pe

  C vs D token-id match: 532/532 = 1.000
  first divergence step: 532

  A vs C token-id match (sampling vs deterministic boundaries): 532/532 = 1.000
  first divergence step: 532

Done.
(.venv) PS F:\Development\superintelligence> python secret_lab_ignore/blomo_port/lab.py --module 6
Loading Bolmo-1B on cpu (dtype=torch.float32)...
Loading weights: 100%|█| 263/263 [00:00<00:00, 2441.79it/s, Materializing param=model.n
Loaded. Parameters: 1,468,911,776

--- Module 6: Prefill fast port (compute removal) ---

[Module 6] Prefill fast port + compute removal proof
  prompt='Language modeling is '
  K=16384
  max_new_tokens=512

Baseline deterministic:
  tokens=532 time=33.62s  boundary_predictor_calls=2
  preview:
Language modeling is a fundamental task in natural language processing (NLP). It is a key component of machine translation, automatic speech recognition, and a variety of other applications. Traditional methods for language modeling typically require a large amount of labeled data, which is often not easily available. In this paper, we propose a novel method for language modeling that uses multitask learning. Our approach uses multiple tasks to learn a distribution over word sequences. We show empirically that the proposed model can effectively pe

Patched deterministic (prefill fast):
  tokens=532 time=33.31s  boundary_predictor_calls=1
  preview:
Language modeling is a fundamental task in natural language processing (NLP). It is a key component of machine translation, automatic speech recognition, and a variety of other applications. Traditional methods for language modeling typically require a large amount of labeled data, which is often not easily available. In this paper, we propose a novel method for language modeling that uses multitask learning. Our approach uses multiple tasks to learn a distribution over word sequences. We show empirically that the proposed model can effectively pe

Token-id match: 532/532 = 1.000
First divergence at step: 532

Compute removal: boundary_predictor_module.forward calls reduced.

Done.
(.venv) PS F:\Development\superintelligence> 