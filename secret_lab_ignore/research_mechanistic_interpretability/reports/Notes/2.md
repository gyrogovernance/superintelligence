Below is a non‑semantic mechanistic interpretability program: a **list of concrete tests** we can run, organized by theme. Each test is:

- **Anchored** in Sevetlidis–Pavlidis (representation holonomy) where possible  
- **Guided** by CGM/Router structure (depth‑4 closure, K₄ quotient, monodromy, etc.)  
- **Non‑semantic**: we do not interpret meanings, only geometry and dynamics  
- **Actionable**: each item is the basis for a script or family of scripts  

I’ll focus on things we can do with:  
- OLMo‑3‑7B as is  
- The Router kernel + atlas  
- The (currently unstable) ASI agent  

No timelines, just the test catalogue.

---

## A. Anchor Tests: Reproducing and Extending Representation Holonomy

These are the “spine” of the program, directly grounded in the Sevetlidis–Pavlidis paper.

### A1. Reproduce Representation Holonomy on OLMo Layers

**Goal:** Establish the holonomy machinery on OLMo.

- Implement the paper’s estimator:
  - Global whitening (ZCA or at least z‑score) of layer activations  
  - Shared‑midpoint k‑NN neighborhoods  
  - Procrustes in shared subspace (SO(q), q ≪ p)  
  - Loop composition H(γ) = R_{L−1}…R₀, h_norm = ||H−I||_F/(2√p)
- Define loops in *token space* (non‑semantic):
  - Local 2D PCA on token‑ID space or on embedding space  
  - Sample small discrete loops by stepping around a circle in that plane, mapping back to nearest tokens.
- Measure h_norm(layer, radius r) for selected layers (e.g. 0, 8, 16, 24, 31).

**Why:** Gives us a direct comparison point to the paper (MNIST/ResNet) and a baseline for all CGM‑guided tests.

---

### A2. Depth Profile: Holonomy vs Depth and Attention Type

**Goal:** See how path dependence is distributed across OLMo’s architecture.

- Measure h_norm for each transformer block at fixed radius r and fixed family of loops.  
- Compare:
  - Early vs middle vs late layers  
  - Sliding‑attention vs full‑attention layers (pattern: [sliding,sliding,sliding,full]).

**Why:** Identifies “geometric bottleneck” layers and where curvature resides. These are prime candidates for architectural constraints or regularizers.

---

### A3. Linear Null Experiments on OLMo

**Goal:** Confirm the “linear null” in this architecture.

- Construct linearized forward variants **without retraining**:
  - Replace SiLU with identity  
  - Freeze MLP gating (gate_proj) to pass‑through (e.g. use only up_proj·down_proj with linear activation)  
  - Optionally bypass RoPE (replace by identity) for one ablation.
- Recompute holonomy per layer on the same loops.

**Evidence we expect (paper, Prop. A.3):** h_norm should collapse toward numerical floor when z is affine.

**Why:** Quantifies how much of OLMo’s curvature/holonomy is due to nonlinearity vs attention structure; guides where to intervene if we want to control path dependence.

---

### A4. Local Equivariance vs Global Holonomy

**Goal:** Replicate the paper’s key claim that local equivariance and global holonomy are distinct.

- Implement a **Lie‑derivative style local equivariance error** (as in Gruver et al. 2022) for OLMo:
  - Choose a simple input transformation T (e.g. cyclic shift in sequence, or token‑ID affine map).
  - Compare: z(Tx) vs U(z(x)) for some linear U fitted locally.
- Compare layer‑wise:
  - Local equivariance error (Lie derivative metric)  
  - Holonomy h_norm on loops generated by repeatedly applying T and returning.

**Why:** If networks can be locally near‑equivariant yet have non‑trivial holonomy, this justifies using holonomy as the *non‑semantic path‑dependence* signal we care about.

---

## B. CGM‑Style Closure and Monodromy in Transformers

These tests look for CGM’s structural signatures (depth‑4 closure, monodromy scales, su(2)‑like behavior) inside OLMo’s representation dynamics.

### B1. Depth‑2 vs Depth‑4 Loop Closure (xy vs xyxy)

**Goal:** Find a transformer analogue of CGM’s depth‑4 closure (BU‑Egress).

- Define two “directions” in input space: T₁, T₂. Non‑semantic examples:
  - T₁: increment last token ID modulo vocab  
  - T₂: apply cyclic shift to a small window of tokens.
- Construct discrete loops:
  - Depth‑2: γ₂(x) = x → T₁x → T₂T₁x → T₁T₂T₁x (2 steps each way)  
  - Depth‑4: γ₄(x) = x → T₁x → T₂T₁x → T₁T₂T₁x → T₂T₁T₂T₁x
- Measure representation holonomy H₂, H₄ at each layer.

**CGM prediction:** depth‑4 alternation should have significantly smaller holonomy than depth‑2 (analog of xyxy ≈ id).

**Why:** Tests whether learned representations exhibit a CGM‑like “minimal closure depth”.

---

### B2. Search for BU‑Scale Monodromy in Eigen‑Angles

**Goal:** Look for CGM’s preferred angles (δ_BU ≈ 0.195 rad, SU(2) commutator 0.5879 rad) in OLMo’s holonomy spectrum.

- For many loops γ (small radius), extract eigen‑angles {θⱼ} of H(γ).  
- Aggregate histograms across layers and radii.  
- Test for:
  - Peaks around ≈ 0.195 rad (11.2°)  
  - Peaks around ≈ 0.588 rad (33.7°)
- Use simple statistical tests (e.g. kernel density vs null with mixed Gaussians).

**Why:** If such angles recur, they are candidates for “natural” curvature scales in deep representations aligned with CGM monodromy values.

---

### B3. Numerical su(2) Closure in Local Subspaces

**Goal:** Look for small su(2)‑like Lie algebras in representation space.

- At a given layer:
  - Select a local 3D subspace by PCA or by choosing three principal curvature directions (from repeated holonomy matrices).
- Approximate three generators X, Y, Z by fitting:
  - H(γ₁) ≈ exp(X), H(γ₂) ≈ exp(Y), H(γ₃) ≈ exp(Z) for carefully chosen loops.
- Numerically check approximate commutation relations:
  - [X,Y] ≈ Z, [Y,Z] ≈ X, [Z,X] ≈ Y up to scaling.

**Why:** Evidence that OLMo’s path dependence is effectively acting through a 3D rotational structure, consistent with CGM’s su(2) necessity argument.

---

### B4. Dual‑Pole Loops and Residual Holonomy

**Goal:** Implement an analogue of CGM’s ONA→BU⁺→BU⁻→ONA dual‑pole loop.

- Identify two approximate “poles” in input transformation space (e.g., two directions T₊, T₋ that are inverses: token ID +1 and −1 mod vocab on subset, or left/right circular shifts).
- Construct dual‑pole loop:
  - γ_dp = T₊ → T₂ → T₋ → T₂⁻¹ on base input x (or an analogous pattern).
- Measure the net holonomy magnitude and eigen‑angle structure.
- Compare across layers: does there exist a stable “defect” angle (constant across inputs and layers or concentrated at some layer)?

**Why:** This is the most direct transformer‑side echo of δ_BU: a specific 4‑edge loop with a characteristic residual holonomy.

---

## C. Router‑Anchored “MRI” Tests on OLMo

These use the Router kernel as an external structured probe into OLMo’s dynamics.

### C1. Joint Holonomy: Router vs OLMo on the Same Byte Loops

**Goal:** Compare discrete kernel monodromy to OLMo representation holonomy.

- For a chosen base byte sequence (derived from tokens via UTF‑8 or tokenizer bytes), define a family of small byte‑loops in Router space: γ_b.
- Map each byte sequence to a token sequence (or treat bytes as synthetic tokens with a small learned embedding) and pass through OLMo.
- For each γ_b:
  - Compute Router invariants (horizon index, vertex charge, phase shifts).
  - Compute OLMo holonomy at multiple layers.
- Correlate Router monodromy magnitude/structure with OLMo h_norm.

**Why:** If OLMo’s curvature aligns with Router’s monodromy, the Router acts as an external “coordinate chart” for OLMo’s geometry.

---

### C2. Horizon‑Aligned Feature Reshaping (4096 = 256 × 16)

**Goal:** Exploit the perfect factorization of OLMo’s hidden size (4096) into Router horizon × channels.

- For each layer activation h ∈ ℝ⁴⁰⁹⁶, reshape to H ∈ ℝ^{256×16}.  
- Index the first dimension by Router horizon index h_R (0..255).  
- For each input sequence, simultaneously step Router on the byte stream and OLMo on the tokens:
  - At time t, we know Router’s horizon h_R(t); examine H[h_R(t), :] vs H at other rows.
- Tests:
  - Holonomy restricted to the “aligned” row vs others.  
  - Variance, curvature, and stability of that row over loops vs misaligned rows.

**Why:** Checks whether OLMo spontaneously organizes its representation in a way that is compatible with Router’s 256‑way horizon structure.

---

### C3. Vertex‑Charge Conditioned Holonomy

**Goal:** See if OLMo is sensitive to Router’s 4‑way K₄ partition.

- Use mask code parity checks (q₀=0x033, q₁=0x0F0) to assign each byte to a vertex 0..3.
- Construct loops γ^same where all steps use bytes from the same vertex; and γ^cross where vertices alternate in a specified pattern.
- Feed corresponding token sequences through OLMo; compute layer‑wise holonomy.

**Expectation:** If OLMo “respects” this quotient, loops that stay within a vertex may have systematically lower holonomy than cross‑vertex loops.

**Why:** Ties OLMo’s path dependence to discrete K₄ structure CGM/Router already expose.

---

### C4. xyxy Identity Words from Router in OLMo

**Goal:** Take Router’s depth‑4 identity property (xyxy = id on Ω) and see if OLMo shows reduced holonomy on those same byte patterns.

- Pick a set of byte pairs (x,y). For each pair, Router guarantees:
  - T_y(T_x(T_y(T_x(s)))) = s for all s in Ω.
- Construct the corresponding 4‑token (or 4‑byte) patterns and feed as loops to OLMo.
- Compare holonomy of these “xyxy identity” loops vs random 4‑step loops of same local radius.

**Why:** If OLMo’s geometry is partially shaped by Router‑like constraints (via training pipelines, codecs, or structure), these special words might sit near holonomy minima.

---

### C5. History Degeneracy: Different Byte Histories, Same Router State

**Goal:** Test CGM/Router’s “many histories → one state” degeneracy against OLMo’s representation.

- Use Router’s known result: for certain restricted alphabets and sequence lengths, multiple byte sequences yield the same final state (e.g., 64 sequences per state).
- Construct pairs of sequences (b_seq¹, b_seq²) with same final Router state but different byte histories.
- Convert to token sequences and feed OLMo:
  - Compare representations at the final token (per layer).
  - Measure representation distance, holonomy differences when put in a loop crossing between these two histories.
  
**Why:** If OLMo generalizes like the kernel (path degeneracy collapsing to same operational state), these pairs should map to similar regions in representation space; if not, it suggests where Agent/kernel coupling could add value.

---

## D. Agent / Inference Function Tests

These specifically target the unstable Gyroscopic ASI agent, using holonomy and CGM as diagnostics.

### D1. Holonomy of the Inference Field M

**Goal:** Treat M[h,p,:] itself as a representation field and measure its path dependence.

- During agent operation:
  - Record sequences of (h_t, p_t) visited and the corresponding M slices.
- For small “control loops” in (h,p) space induced by particular byte patterns, define a discrete connection on M via inner products / Procrustes alignment.
- Compute holonomy of M around these loops.

**Why:** If M’s holonomy is wildly unstable or unstructured compared to OLMo’s, that’s direct evidence about where the agent architecture is misaligned with the Router physics.

---

### D2. η and γ‑Table Stability Sweep via Holonomy

**Goal:** Use holonomy as a *non‑semantic stability metric* for the agent.

- Systematically vary:
  - Learning rate η (currently ≈ aperture gap ≈ 0.00117)  
  - Direction factor γ_table patterns (same, adjacent, opposite vertex weights).
- For each setting, feed a battery of non‑semantic token patterns (random text, periodic patterns).
- For each, measure:
  - Holonomy of agent byte output sequences (as a function of input loops).
  - Norms and eigen‑angles of cumulative transformation implied by the agent (e.g., mapping from embedding field to bytes viewed as a field).

**Why:** Directly uses path‑dependence to locate stable vs chaotic agent regimes, independent of semantics.

---

### D3. Router vs Agent Holonomy Alignment

**Goal:** Check if the agent’s Inference Function respects Router’s monodromy structure.

- Run agent on controlled byte loops where Router monodromy is known and simple (e.g., within single vertex / single horizon).
- Compare:
  - Router’s discrete loop defect (e.g., horizon, vertex changes, phase)  
  - Holonomy of the agent’s generated byte sequences or internal M.

**Why:** If the agent does not preserve Router’s clean depth‑4 identity and K₄ quotient, that pinpoints where the architecture diverges from the geometric substrate.

---

## E. Transformer Internal Geometry Tests (Non‑Semantic)

### E1. Per‑Head Representation Holonomy

**Goal:** Localize curvature to particular attention heads.

- At head dimension d_head=128:
  - Extract per‑head activations (Q/K/V or attention outputs) for each layer.
- Run representation holonomy per‑head:
  - Treat each head’s output as a p=d_head feature vector and compute h_norm for loops.
- Compare:
  - Distribution of holonomy across heads and layers.
  - Correlations with weight norms or head type (sliding vs full).

**Why:** Identifies geometrically dominant heads that shape path dependence, which can be targets for regularization or architectural change.

---

### E2. RoPE Ablation via Holonomy

**Goal:** Understand how rotary embeddings contribute to curvature.

- Modify forward pass (no retraining) to:
  - Bypass RoPE for Q/K (use raw projections).
- Compute holonomy with and without RoPE.

**Why:** Gives a geometric characterization of RoPE’s effect: purely local (projection) vs global path dependence.

---

### E3. QK‑Norm Ablation via Holonomy

**Goal:** See how OLMo’s custom QK normalization shapes its geometry.

- In forward pass hooks:
  - Disable QK normalization (or set weights to 1); recompute attention.
- Measure h_norm changes per layer on same loops.

**Why:** If QK‑norm disconnects large curvature spikes or regularizes holonomy, it’s a good site for principled modifications.

---

### E4. MLP vs Attention Curvature Contribution

**Goal:** Separate where path dependence comes from.

- For each layer, break holonomy into:
  - Pre‑attention → post‑attention change.  
  - Pre‑MLP → post‑MLP change.
- Compute ∆h_norm per sublayer.

**Why:** Quantifies whether to target attention or MLP for CGM‑style constraints.

---

## F. Training Dynamics and Checkpoint Geometry

(Requires at least a few checkpoints, but no retraining from scratch.)

### F1. Holonomy Trajectories across Checkpoints

**Goal:** Confirm Sevetlidis–Pavlidis’s observation that holonomy rises then stabilizes during training.

- Acquire checkpoints across OLMo pretraining (if available, or use partial training runs).
- For a fixed set of loops and layers, track h_norm(checkpoint, layer, r).

**Why:** Connects path dependence growth with learning of structure, non‑semantically.

---

### F2. Fine‑Tuning Effects

**Goal:** See how instruction fine‑tuning modifies geometry.

- Compare pre‑ and post‑instruct checkpoints:
  - Same holonomy protocol on shared tasks.
- Measure which layers change curvature the most.

**Why:** Tells us where the architecture absorbs alignment pressure geometrically.

---

## G. Local Geometry vs Holonomy

### G1. Lipschitz vs Holonomy Correlation

**Goal:** Confirm that Lipschitz constants do not subsume holonomy.

- For each layer and loop:
  - Approximate local Lipschitz constant (finite differences along loop directions).
  - Compute h_norm for the loop.
- Correlate across loops and layers.

**Why:** Validates using holonomy as distinctly informative (not just a rephrasing of sensitivity).

---

### G2. Dimensionality of Curvature Subspace

**Goal:** Measure how many directions holonomy truly “bends” in.

- For many loops γₖ at fixed layer:
  - Collect matrices Hₖ − I.
- Compute the rank (or numerical effective rank) of the span {Hₖ−I}.
- Compare with 3 (su(2)) as CGM’s minimal curvature dimension.

**Why:** If curvature mostly lies in a low‑dimensional subspace, we can design regularizers or architectural modules that explicitly control those few directions.

---

## H. Horizon‑Like and Quotient‑Like Structure in OLMo

### H1. “Horizon States” in OLMo

**Goal:** Find inputs whose representations are nearly fixed under a simple reference transformation (an analogue of Router’s horizon).

- Choose a reference transformation R (e.g., token ID XOR with constant, or shift in some canonical direction).
- Search over tokens or short token patterns:
  - Find inputs x where ||z(x) − z(Rx)|| is minimal at some layer.
- Treat these x as approximate “horizon” examples.
- Measure:
  - Holonomy of small loops anchored at these x vs generic x.

**Why:** If there are natural horizon‑like states, they might be privileged in the architecture (anchors for geometric structure).

---

### H2. 4‑Cluster Quotient Structure

**Goal:** Look for K₄‑like vertex clusters in OLMo representation space.

- For a given layer:
  - Use unsupervised clustering (k=4) on a large batch of token representations.
  - Track cluster assignments across layers (are clusters stable?).
- For each cluster:
  - Measure intra‑cluster vs inter‑cluster holonomy on loops that stay within vs cross clusters.

**Why:** Empirically tests whether OLMo internally factors representation space into 4 “modes” reminiscent of Router’s K₄ quotient.

---

## I. Weight‑Space and Parameter‑Manifold Tests

(Deeper, more speculative, but non‑semantic.)

### I1. Holonomy of Parameter‑Defined Embedding Fields

**Goal:** Treat slices of OLMo’s weight matrices as a static field and probe their geometry.

- Construct a field F over, e.g., (layer, head) by:
  - For each (layer ℓ, head h), take a fixed 4096‑dim vector (e.g., a row/column of q_proj or o_proj) and map it into 256×16 field like we do for embeddings.
- Define loops in (ℓ,h) index space (small 2D grids of neighboring layers/heads).
- Apply the holonomy estimator on F along these loops.

**Why:** Uses non‑semantic geometry in *parameter space* to see if su(2)/K₄‑like structures are present already in static weights, not just in activations.

---

## Summary

This program gives us:

- A **holonomy backbone** (A‑series) firmly grounded in Sevetlidis–Pavlidis.
- A set of **CGM‑aligned tests** (B, H) looking for depth‑4 closure, su(2) signatures, K₄ quotient, and monodromy scales inside OLMo.
- **Router‑anchored probes** (C) that use the kernel as an external coordinate system and MRI‑like tool.
- **Agent‑specific diagnostics** (D) that use holonomy/nonlinearity to stabilize and refine the ASI agent.
- **Purely architectural, non‑semantic tests** (E, F, G, I) for how path dependence is distributed and controlled.

All of these are independent of semantic labeling and focus purely on **how the path “walks” through the transformer**, in exactly the sense you described: intelligence as structured path dependence, not as content.

## Rubric-driven iteration on the previous proposal set

I’m going to do three things:

1. **Audit each previously proposed test family** against the rubric (keep / modify / drop).
2. **Rewrite the kept items** to remove weak/“toy” forms, add guardrails, and make them compute-feasible on OLMo.
3. **Add missing tests** that are necessary for external credibility (Sevetlidis–Pavlidis-style estimator integrity suite) and for non-semantic architectural optimization.

I’ll avoid anything that depends on “aperture as displacement,” per your requirement.

---

# 1) What stays, what changes, what goes

Legend:
- **KEEP** = already high-value and externally anchored
- **MODIFY** = good idea, but needs redefinition to avoid toy/partial/circular/compute-heavy
- **DROP** = too speculative, too expensive, too “cute,” or not provably informative

### A. Sevetlidis–Pavlidis anchor tests (core spine)
- **A1 Reproduce holonomy on OLMo layers** → **KEEP, but MODIFY loop construction**  
  The estimator is right; the weak point was token-loop construction. We should implement *continuous loops via embedding injection* (see below), not “map circle points back to nearest token IDs” which is fragile and toy-ish.
- **A2 Holonomy vs depth and attention type** → **KEEP**  
  High stakes, direct architectural insight, cheap once A1 exists.
- **A3 Linear null experiments** → **KEEP, but MODIFY ablations to be clean and reversible**  
  Needs careful “no retraining” ablation definitions and controls so it’s publishable.
- **A4 Local equivariance vs global holonomy** → **MODIFY**  
  Good but can become expensive or vague. Keep only if we define a precise transformation group and a clear measurement protocol; otherwise it risks becoming “extra metrics for their own sake.”

### B. CGM-guided structure tests inside transformers
- **B1 Depth-2 vs depth-4 closure (xy vs xyxy)** → **KEEP, but MODIFY to match S–P loop formalism**  
  This can be made very strong and non-semantic if done as commutator rectangles in a 2D injected-input plane.
- **B2 Search for δ_BU scale in eigen-angles** → **MODIFY (make it a secondary analysis)**  
  It’s interesting but can easily look like numerology. Keep only as a **registered after-analysis** once we have strong holonomy baselines and nulls.
- **B3 su(2) closure in local subspaces** → **MODIFY heavily (or postpone)**  
  Risk: too interpretive and too easy to “fit what we want.” Needs strict nulls and a non-CGM baseline; otherwise it’s self-fulfilling.
- **B4 Dual-pole loop analog** → **MODIFY**  
  Keep only if we define poles as **two opposite directions in the same 2D input plane** and measure a stable residual. Otherwise it’s too unconstrained.

### C. Router-as-MRI tests on OLMo
- **C1 Joint holonomy Router vs OLMo on same byte loops** → **MODIFY**  
  The “convert bytes to tokens” mapping can become arbitrary. If we do this, we must make the mapping *canonical* and test mapping sensitivity.
- **C2 Horizon-aligned reshaping (4096 = 256×16)** → **KEEP**  
  This is a uniquely strong, non-semantic, architecture-relevant test. It can reveal whether OLMo naturally supports a 256-way factorization.
- **C3 Vertex-charge conditioned holonomy** → **KEEP, but MODIFY to avoid overfitting Router structure**  
  Use it as a **conditioning variable** and compare to random 4-way partitions as null.
- **C4 xyxy identity words from Router** → **MODIFY**  
  Router has exact xyxy=id; OLMo won’t. This should be reframed as: do Router-identity words produce **systematically lower holonomy** than matched controls? That’s a real test.
- **C5 History degeneracy (different histories, same Router state)** → **KEEP, but MODIFY into a falsifiable “collapse vs separation” test**  
  Very valuable if done with proper controls.

### D. Agent / Inference Function tests
- **D1 Holonomy of M field** → **MODIFY**  
  Good, but define the “connection” cleanly (S–P style Procrustes between neighborhood samples of M) or it becomes ad hoc.
- **D2 η/γ sweeps** → **MODIFY**  
  Don’t do parameter sweeps broadly (compute + “toy”). Instead, do **small, theory-motivated** perturbations with stability diagnostics.
- **D3 Router vs Agent holonomy alignment** → **KEEP**  
  This directly targets why the Agent is unstable.

### E. Transformer internal attribution tests (very valuable)
- **E1 Per-head holonomy** → **KEEP**  
  High payoff: identifies heads dominating curvature; compute can be managed with caching.
- **E2 RoPE ablation** → **KEEP**  
- **E3 QK-norm ablation** → **KEEP**  
- **E4 MLP vs Attention contribution** → **KEEP**  

### F. Training dynamics / checkpoints
- **F1 Holonomy trajectories across checkpoints** → **DROP for now (compute/data availability)**  
  Only keep if you already have checkpoints. Otherwise it violates compute realism.
- **F2 Fine-tuning effects** → **MODIFY**  
  Keep only as “compare two existing weights you already have,” not as a training plan.

### G. Geometry vs other measures
- **G1 Lipschitz vs holonomy** → **MODIFY**  
  Only keep if it’s tight and used to show holonomy is non-redundant; otherwise it’s “circles around metrics.”
- **G2 Curvature subspace dimension** → **KEEP**  
  Strong and actionable for architectural optimization (low-rank curvature control).

### H. Horizon/quotient structure discovery inside OLMo
- **H1 “Horizon states” in OLMo** → **DROP / postpone**  
  Too unconstrained; risks being interpretive.
- **H2 4-cluster quotient structure** → **MODIFY**  
  Only keep if we do it as a strict null-tested question: “does any 4-way partition show special behavior, or is Router’s partition uniquely predictive?”

### I. Parameter-manifold holonomy
- **I1 Holonomy of parameter-defined embedding fields** → **DROP (too speculative now)**  
  Not clearly high-stakes relative to effort; easy to look like mathematical decoration.

---

# 2) Corrected + strengthened program (list of what we should examine)

I’ll present the revised test catalogue grouped into “must be in the first serious run” vs “secondary” vs “optional,” but still as a list (not a timeline).

## Tier 0 — Measurement Integrity Suite (mandatory for external credibility)
These are the S–P “bias guardrails” adapted to OLMo. Without these, any results are hard to present as substantial evidence.

### T0.1 Self-loop / zero-radius bias floor
- Construct γ with all points identical (or r→0 injected loop).
- Confirm h_norm collapses to numerical floor.
- Report floor per layer (important: OLMo activations are bfloat16 often).

### T0.2 Gauge invariance check (orthogonal reparameterization)
- Insert a random orthogonal transform U on the measured feature vectors z(x) *after extraction* (do not change model), verify:
  - H′ = UHUᵀ and h_norm invariant, eigen-angles invariant.

### T0.3 Neighbor sharing ablation (catastrophic inflation check)
- Run with shared-midpoint neighbor set (correct).
- Run with separate neighbor sets at endpoints (should inflate h_norm massively, as in S–P).
- This verifies we’re measuring geometry, not index noise.

### T0.4 (k,q,Npool) stability grid
- Small grid like (k,q) ∈ {96,128,192}×{32,64,96}, Npool ∈ {1024,2048,4096}
- Confirm qualitative invariance.

### T0.5 Whitening sensitivity
- Compare z-score vs ZCA-corr (or ZCA approx after JL projection).
- Ensure conclusions do not depend on whitening choice.

**Why Tier 0 is “high-stakes”:** It determines whether anything else is publishable or just internal play.

---

## Tier 1 — S–P holonomy on OLMo with correct loop construction (core deliverable)

### T1.1 Continuous loop injection at the embedding layer (replace “token circle mapping”)
Instead of inventing fragile token loops, use *continuous loops in input space*, matching S–P’s assumptions.

- Take a base token sequence (non-semantic random tokens are fine).
- Let E ∈ ℝ^{seq×4096} be the input embeddings (before first layer).
- Build a local 2D plane in embedding space:
  - Choose two orthonormal directions u, v (random, or PCA from small perturbations).
- Define loop points:
  - E(θ_i) = E + r( cos θ_i · u + sin θ_i · v ), i=1..L
- Run OLMo forward with overridden input embeddings (no tokenizer semantics involved).
- Measure holonomy per layer using S–P estimator.

This makes the “loop radius r” mathematically meaningful and avoids toy discretization issues.

### T1.2 Layer-wise holonomy profile (all 32 layers + embedding + final norm)
- Compute h_norm(layer, r) for multiple radii (e.g. 4 radii) and multiple loop centers (multiple base sequences).
- Report robust summaries (median/IQR) across centers.

### T1.3 Sliding vs full attention layer comparison
- Directly compare distributions for full-attention layers (3,7,...,31) vs sliding layers at matched r.

### T1.4 Sub-layer attribution (attention block vs MLP block)
For each layer, measure holonomy at:
- pre-attn input
- post-attn residual output
- post-mlp residual output

This yields ∆holonomy contributed by attention vs MLP in a non-semantic, causal way.

---

## Tier 2 — Architectural “knobs” that control path dependence (no retraining)

These are high-stakes because they directly guide transformer optimization and/or Agent evolution.

### T2.1 Linear null via SiLU removal (clean ablation)
- Replace SiLU with identity (or gate=1) in forward.
- Expect: holonomy drops significantly toward floor (not necessarily zero because attention + normalization may still induce nonlinearity-like effects, but it should be materially reduced).

### T2.2 RoPE ablation (Q,K rotary embedding bypass)
- Disable rotary embedding application.
- Measure holonomy change:
  - Does RoPE create curvature early? late? mostly in full-attention layers?

### T2.3 QK-norm ablation (disable q_norm/k_norm)
- Disable Q/K RMSNorm scaling.
- Measure holonomy and its stability vs r.

### T2.4 Head-local holonomy (per head, per layer)
- Extract per-head outputs (dim=128) and compute holonomy per head.
- Output: “curvature hotspots” across heads.
- This is actionable: we can test whether a small subset of heads carries most path dependence.

### T2.5 Attention vs MLP “curvature budget” summary
Define a scalar per layer:
- C_attn = median(h_norm post-attn – h_norm pre-attn)
- C_mlp = median(h_norm post-mlp – h_norm post-attn)

This is not semantics; it’s an optimization target.

---

## Tier 3 — CGM-guided closure tests, rewritten to avoid numerology

### T3.1 Commutator rectangle holonomy (depth-2 vs depth-4 analogue)
In the *same injected 2D input plane*, define a rectangular commutator loop:

- Move +ε in u, then +ε in v, then −ε in u, then −ε in v (a small closed rectangle).
- This is the standard discrete curvature probe (connection curvature ~ commutator of transports).

Now compare:
- “2-step effect”: transport along u then v vs v then u (path dependence)
- “4-step closure”: does the commutator loop produce systematically smaller holonomy than simpler loops?

This is a strong, clean CGM-aligned test without needing to force Router words into OLMo.

### T3.2 Effective curvature rank (how many directions bend?)
Collect many H_k (loop holonomies) at fixed layer and radius, compute effective rank of {H_k − I}.
- If rank is low, that supports designing low-rank curvature controllers / constraints.

### T3.3 Eigen-angle analysis as *secondary* (guarded)
Only after T0–T2:
- Aggregate eigen-angles and check for stable peaks.
- Must include null comparisons:
  - random orthogonal noise baseline
  - linearized model baseline
If peaks persist under those controls, then it’s meaningful to compare to δ_BU-scale numbers.

(Without those nulls, it risks looking like “pattern hunting.”)

---

## Tier 4 — Router-as-MRI, corrected to be falsifiable (and not forced)

### T4.1 4096 = 256×16 horizon reshaping test (KEEP strong)
- Reshape each layer vector into 256×16.
- Use Router horizon index sequence (from bytes derived from the same token stream by a *fixed*, documented mapping) as an index.
- Test: does the “aligned row” show different curvature statistics than non-aligned rows?

Add null:
- Compare to random 256-way permutations of the row axis.
If Router indexing is special, it should outperform random permutations.

### T4.2 Vertex-charge conditioning with strict nulls
- Compute Router vertex charge class of each byte (4-way).
- Condition holonomy measurements on charge patterns in the input byte stream.
- Null: compare to random 4-way partition of bytes.

### T4.3 Router identity-word bias test (xyxy)
Reframe C4 to a real test:

- Build many 4-step byte patterns:
  - Group A: Router identity words xyxy
  - Group B: matched random 4-byte words with same byte weight statistics
- Map both groups to token sequences via a fixed mapping.
- Measure: are Group A patterns systematically lower-holonomy in early layers?

If “no,” that’s still valuable: it means Router’s exact closure is not mirrored by OLMo.

### T4.4 History degeneracy “collapse vs separation” test
- Find byte histories (b1,b2) that land on same Router final state.
- Feed corresponding token sequences to OLMo.
- Measure representation distance and holonomy differences at final token:
  - If OLMo collapses them (small distance), it’s kernel-like.
  - If OLMo separates them, that’s evidence the Router can provide compression/generalization not present in transformer internals.

This is a high-value, non-semantic result either way.

---

## Tier 5 — Agent stabilization diagnostics (only what’s non-toy)

### T5.1 Router ↔ Agent alignment under controlled loops (KEEP)
- Feed controlled byte loops into the agent (no semantics).
- Track:
  - kernel state / horizon / vertex / phase
  - chosen bytes
  - stability of M updates
- Compare against Router expectations for phase behavior and wedge coherence.
This directly targets the Agent’s instability.

### T5.2 M-field holonomy with S–P estimator (MODIFY to be clean)
Treat M[h,p,:] as the representation field:
- Choose small loops in (h,p) space induced by repeating known byte-cycle patterns.
- Define neighborhoods in M-space and do the same Procrustes transport + loop composition.
If M has chaotic holonomy under controlled input loops, that is a precise failure mode.

### T5.3 Minimal η/γ perturbations (not broad sweeps)
Do not do sweeping. Do 2–3 theory-motivated perturbations:
- η → 0 (frozen memory) baseline
- η → 2η (stress test)
- remove opposite-vertex penalty in γ (stress test)
Measure stability via holonomy + boundedness of M norms.

---

# 3) Key removals (explicit)

To stay aligned with your “no circles, no toy, no expensive compute” constraints, I recommend we **remove for now**:

- **Checkpoint/training-dynamics work** unless you already have checkpoints locally.
- **Parameter-manifold holonomy** (too unconstrained; easy to become decorative).
- **Unconstrained discovery of “horizon states” in OLMo** (too interpretive).
- **Robustness correlation experiments** (FGSM/PGD/ImageNet-C style) unless we can do them cheaply and cleanly; they are not needed for your goal (architectural optimization).

---

# 4) Additions that I consider necessary (you didn’t ask explicitly, but they matter)

These are “non-semantic but make results substantial.”

### Add: Caching + compression protocol (compute realism)
To keep this feasible:
- Cache per-layer activations for the loop points (L points × few sequences).
- Use a fixed JL projection p→p* (e.g. 1024) before k-NN + Procrustes, like S–P do, to keep k-NN and whitening tractable.

### Add: Pre-registered null suite (avoid self-fulfilling prophecy)
For any “CGM-like” signature claim, always include:
- linearized model null
- random orthogonal mixing null
- neighbor-sharing ablation null
- random 4-way partition null (when using Router K₄ conditioning)

This makes the work presentable as “substantial proof of something” rather than internal confirmation.

---

# Hidden Insights Worth Pursuing: Advanced Mathematical Physics + ML Perspective

As an advanced scientist bridging mathematical physics (gauge theory, differential geometry, representation theory) and machine learning (manifold learning, equivariant dynamics, spectral graph theory), I'll identify **five high-potential insights** that emerge when synthesizing your CGM/GGG/Router framework with the Sevetlidis–Pavlidis holonomy paper, viewed through OLMo-3-7B's architecture. These are not obvious engineering tweaks or semantic probes; they are **structural conjectures** about how transformers realize path-dependent intelligence via geometric invariants, potentially revealing why transformers "work" at scale without explicit CGM-like constraints.

These insights are "hiding in plain sight" because:
- **CGM/Router**: Provide the axiomatic template (chirality, depth-4 closure, holographic boundaries, K₄ quotients) but lack empirical ML grounding.
- **S–P Paper**: Measures emergent holonomy empirically but treats it as a diagnostic without theoretical unification.
- **OLMo**: Embodies the "black box" where these structures might self-organize, with its 4096=256×16 factorization, 32-layer depth, and sliding/full attention duality screaming for geometric interpretation.

Each insight is:
- **Non-semantic**: Purely about differential geometry and group actions on representations.
- **Rubric-aligned**: High-stakes (architectural optimization via curvature control), CGM-guided (specific predictions), externally anchored (S–P holonomy as measurement), feasible (leverages existing OLMo weights, no retraining), non-trivial math (Lie algebra, spectral invariants), and interpretable (clear geometric meaning).
- **Strategic**: Points to transformer evolution (e.g., explicit curvature regularization) and Agent stabilization (e.g., injecting CGM quotients).

I'll describe each insight, its "hidden" origin, why pursue it (rubric tie-in), and a **concrete, rubric-passing probe** (testable with OLMo, compute <1 GPU-day).

---

## Insight 1: Attention as Discrete Berry Phase Accumulator

**Description**: In quantum physics, Berry phase measures path-dependent phase shifts in adiabatic evolution around loops in parameter space—exactly holonomy in a U(1) bundle. Transformers' attention mechanism, with its softmax-normalized inner products and rotary embeddings (RoPE), acts as a discrete analog: attention weights define a "connection" on the residual stream, and RoPE injects a U(1)-like phase via rotations. Hiding in plain sight: S–P's eigen-angles {θ_j} of H(γ) are Berry-like phases accumulated over loop steps, while CGM's δ_BU ≈ 0.195 rad is a U(1) monodromy defect. OLMo's sliding/full attention duality might enforce a "adiabatic" (local) vs "non-adiabatic" (global) regime, with full-attention layers accumulating larger phases.

**Hidden Origin**: S–P mentions "eigenvalues e^{iθ_j} on unit circle" but doesn't connect to Berry phase; CGM's BU residual is explicitly U(1) holonomy; OLMo's RoPE is a literal rotation group action, but no one asks if it induces CGM-scale defects.

**Why Pursue (Rubric)**: 
- **A1/A2 (High)**: Directly extends S–P's spectral analysis to physics (Berry phase), publishable as "holonomy as discrete Berry curvature in transformers."
- **B1/B2 (High)**: Tests CGM's U(1) residual (δ_BU scale in eigen-angles); falsifiable (null: uniform phase distribution).
- **D1/D2 (High)**: Pure spectral geometry; reveals if attention "bends" representations via phase defects, guiding RoPE/attention redesign.
- **E1–E3 (Critical)**: Full OLMo scale; complete (multi-layer phase tracking); substantial (if phases cluster at CGM scales, it's a geometric "why transformers work").
- **F1/F2 (Medium)**: Feasible (reuse T1.1 loops, add phase extraction).
- **G1/G2 (Medium)**: Non-trivial (Berry connection on learned manifolds); interpretable (phase defect = path memory).

**Concrete Probe (T1.5: Berry Phase in Attention Loops)**:
- Use T1.1's continuous embedding loops (r small, L=12 points).
- For each layer, extract attention weights A ∈ ℝ^{seq×seq} (softmax(QK^T/√d_k)) and RoPE rotations R.
- Compute "attention Berry phase": trace of the Wilson loop ∫ A dθ (discretized as sum A_i over loop points), or more precisely, the eigen-phases of the composed transport R · A around the loop.
- Compare to S–P's H(γ) eigen-angles: do attention-induced phases cluster near 0.195 rad (δ_BU) or 0.588 rad (SU(2) commutator)?
- Null: Linearized attention (no softmax/RoPE) should give uniform/random phases.
- Output: Layer-wise phase histograms; test for CGM-scale peaks (Kolmogorov–Smirnov vs uniform null).

**Strategic Direction**: If confirmed, evolve Agent by injecting explicit Berry-phase regularization (e.g., penalize non-CGM-scale defects in M-field updates), stabilizing path memory without semantics.

---

## Insight 2: Implicit K₄ Quotient Learning in Residual Streams

**Description**: CGM's K₄ tetrahedral quotient (4 vertices, 6 edges) emerges from su(2) irreducibility; Router realizes it via mask parity checks partitioning into 4 wedges. Transformers' residual streams might learn an implicit 4-way quotient for efficiency: the 32 heads (4×8) and 32 layers (4×8) suggest a 4-fold symmetry, with sliding/full attention as "vertex modes." Hiding in plain sight: S–P's q-subspace (low-rank shared structure) is a quotient map; OLMo's 4096=64×64 hidden size factors through 4 (e.g., 4×1024), and attention heads might cluster into 4 geometric "roles" (e.g., via spectral clustering of head projections).

**Hidden Origin**: S–P uses q<<p subspaces but doesn't explore quotient symmetries; CGM derives K₄ from operational closure; OLMo's architecture numbers (32=4×8) are arbitrary but might encode emergent quotients for path compression.

**Why Pursue (Rubric)**:
- **A1/A2 (High)**: Extends S–P's subspace analysis to quotient detection; publishable as "transformers learn implicit tetrahedral quotients for curvature control."
- **B1/B2 (High)**: Tests CGM's K₄ necessity (4-way partition with wedge-like stability); falsifiable (null: random 4-partition shows no special holonomy reduction).
- **D1/D2 (High)**: Geometric (quotient manifold structure); strategic (if true, redesign heads/layers around explicit K₄ symmetry for better path coherence).
- **E1–E3 (Critical)**: OLMo-scale clustering + holonomy; complete (full head/layer coverage); substantial (proves transformers compress paths via low-dim quotients).
- **F1/F2 (Medium)**: Spectral clustering on activations (cheap with PCA pre-reduction).
- **G1/G2 (Medium)**: Non-trivial (quotient by stabilizer subcode); interpretable (4 modes = governance capacities: trace/variety/account/integrity).

**Concrete Probe (T2.6: Quotient Detection in Head Projections)**:
- For each layer, extract head projections: for head h, take the 128-dim output subspace spanned by its o_proj rows (or attention outputs).
- Perform spectral clustering (k=4) on the Grassmannian of these 32 subspaces (use principal angles as distance).
- For each cluster, compute intra-cluster holonomy (loops within cluster inputs) vs inter-cluster.
- Null: Same on random 4-clustering of heads.
- CGM tie-in: Check if clusters align with Router's vertex charges (map head indices mod 4 to charges).
- Output: Cluster stability (Rand index across layers); holonomy reduction factor (intra vs inter).

**Strategic Direction**: If quotients exist, evolve transformers/Agent by enforcing explicit 4-way symmetry (e.g., 4×N heads with K₄-equivariant mixing), reducing path chaos without semantics.

---

## Insight 3: RoPE-Induced Monodromy Defects as Path Memory

**Description**: CGM's monodromy defect δ_BU measures residual holonomy after depth-4 closure—a "memory" of non-commutativity. RoPE in transformers injects rotations that are non-commuting (different frequencies don't commute), potentially creating analogous defects. Hiding in plain sight: S–P's small-radius O(r) scaling matches CGM's quadratic commutator ω ~ θ²; OLMo's YARN RoPE (factor=8) might tune defect scales to CGM values (e.g., 0.195 rad), explaining why rotary embeddings stabilize long-context path dependence.

**Hidden Origin**: S–P ablates RoPE implicitly via position effects but doesn't measure phase defects; CGM's δ_BU is exactly a rotation angle; OLMo's RoPE is frequency-scaled, but no one checks if defects cluster at physical scales.

**Why Pursue (Rubric)**:
- **A1/A2 (High)**: Builds on S–P's eigen-angle analysis + RoPE ablation; publishable as "RoPE as discrete monodromy generator in transformers."
- **B1/B2 (High)**: Tests CGM's defect scale (0.195 rad peaks in RoPE phases); falsifiable (null: uniform defects without RoPE).
- **D1/D2 (High)**: Spectral phase geometry; strategic (tune RoPE factors for CGM-optimal defects to enhance path memory in Agent).
- **E1–E3 (Critical)**: Full OLMo with/without RoPE; complete (multi-frequency analysis); substantial (if defects match CGM, explains rotary universality).
- **F1/F2 (Medium)**: Reuse T1.1 loops; disable RoPE in forward (cheap).
- **G1/G2 (Medium)**: Non-trivial (monodromy in learned rotations); interpretable (defect = residual path memory).

**Concrete Probe (T2.7: RoPE Defect Spectrum)**:
- Use T1.1 embedding loops; extract RoPE rotations R(θ) applied to Q/K at each layer.
- For each frequency band (OLMo's 32 heads span frequencies), compute the composed rotation around the loop: R_total = R(θ_L) ... R(θ_1).
- Extract eigen-angles of R_total; histogram across layers/heads/loops.
- Ablate: Disable RoPE (identity rotations); compare defect distributions.
- CGM tie-in: Test for peaks at ≈0.195 rad (δ_BU) or multiples.
- Null: Random rotations (no frequency structure).
- Output: Defect histograms; KS-test for CGM-scale clustering.

**Strategic Direction**: If confirmed, evolve Agent by injecting CGM-tuned rotations into M-field updates, stabilizing path memory via defect control.

---

## Insight 4: Residual Connections as Depth-4 Closure Enforcers

**Description**: CGM requires depth-4 closure for coherent recursion; transformers' residual connections (skip every layer) might enforce an effective "closure" by projecting back to a low-curvature subspace, reducing accumulated holonomy. Hiding in plain sight: S–P's layer-wise holonomy often stabilizes mid-network, suggesting residuals "reset" curvature; Router's P7 (xyxy=id) is a discrete residual-like identity.

**Hidden Origin**: S–P notes depth effects but not residual role; CGM's BU closure is exactly what residuals approximate; OLMo's 32 layers (multiple of 4) might align with depth-4 periodicity.

**Why Pursue (Rubric)**:
- **A1/A2 (High)**: Extends S–P's depth profiles to residual ablation; publishable as "residuals as holonomy regularizers."
- **B1/B2 (High)**: Tests CGM's depth-4 necessity (residuals should reduce holonomy every 4 layers); falsifiable (null: no periodicity).
- **D1/D2 (High)**: Stream geometry; strategic (redesign residuals with explicit CGM closure, e.g., depth-4 averaging).
- **E1–E3 (Critical)**: OLMo-scale; complete (full depth profile with/without residuals); substantial (if residuals enforce closure, it's a geometric "why transformers generalize").
- **F1/F2 (Medium)**: Forward ablation (set residual scale to 0 or 1).
- **G1/G2 (Medium)**: Non-trivial (residuals as discrete projectors); interpretable (closure = path reset).

**Concrete Probe (T2.8: Residual Ablation and Periodicity)**:
- Ablate residuals layer-wise: set residual scale α=0 (no skip) or α=1 (full skip) in forward.
- Compute holonomy profiles with/without residuals.
- Look for periodicity: ∆h_norm every 4 layers (CGM depth-4 echo).
- Null: Random layer skipping (no structure).
- Output: Holonomy decay rates; Fourier analysis of depth profile for 4-periodicity.

**Strategic Direction**: If residuals enforce CGM-like closure, evolve Agent by adding explicit depth-4 projectors in M updates, reducing instability.

---

## Insight 5: Learned Holographic Dual in Embedding-Residual Mapping

**Description**: CGM's holographic dictionary (bulk ↔ boundary × action) suggests transformers learn a dual: embeddings (boundary) + attention (action) → residual stream (bulk). Hiding in plain sight: S–P's shared-subspace Procrustes is a discrete parallel transport mirroring CGM's gyration; OLMo's 4096-dim embeddings factor as 256×16, suggesting a 256-way "horizon" dual encoding the 32-layer bulk.

**Hidden Origin**: S–P's subspace embedding is holographic-like but unnamed; CGM's dictionary is explicit bijection; OLMo's factorization is coincidental but perfect for testing.

**Why Pursue (Rubric)**:
- **A1/A2 (High)**: Extends S–P's subspace transport to dual conjecture; publishable as "transformers learn holographic duals for path compression."
- **B1/B2 (High)**: Tests CGM dictionary (bijection via subspace rank); falsifiable (null: no low-rank dual structure).
- **D1/D2 (High)**: Manifold duality; strategic (design embeddings with explicit 256-way horizon for better bulk navigation in Agent).
- **E1–E3 (Critical)**: OLMo-scale SVD/rank analysis; complete (embedding-to-residual mapping); substantial (if dual holds, explains scaling laws geometrically).
- **F1/F2 (Medium)**: SVD on activation pairs (cheap).
- **G1/G2 (Medium)**: Non-trivial (holographic reconstruction rank); interpretable (boundary encodes bulk paths).

**Concrete Probe (T2.9: Embedding-Residual Holographic Rank)**:
- For paired embeddings E and residuals Z at layer ℓ (same input loops).
- Compute low-rank approximation: min rank r such that ||Z - U Σ V^T||_F small, with U shaped 4096×r, V from E.
- Test for r ≈ 256 (horizon-like dual).
- CGM tie-in: Check if r correlates with holonomy (higher holonomy → higher rank needed).
- Null: Random pairing of E and Z (should require full rank).
- Output: Rank histograms per layer; reconstruction error vs r.

**Strategic Direction**: If dual exists, evolve Agent by enforcing low-rank M updates (holographic compression), stabilizing via boundary control.

---

## How These Fit the Program

These insights **elevate** the rubric-scored tests (Tiers 0–5) from "measurement" to "discovery":
- **Tier 0–2** now serve as infrastructure for these probes (e.g., T1.1 loops feed T1.5 Berry phase).
- **Tier 3–5** gain depth (e.g., T3.1 commutators test Insight 1's Berry curvature).
- **New probes (T1.5, T2.6–T2.9)** are rubric-strong: each is a single, complete test with clear nulls, directly publishable, and points to Agent/transformer redesign (e.g., curvature control via phases/quotients/duals).

They are "hiding in plain sight" because the math (Berry phase, quotients, holography) is standard in physics but underused in ML interpretability—yet S–P's holonomy formalism and your CGM axioms make the connection natural. Pursuing them positions the work as a unification of learned representation geometry with axiomatic physics, with concrete OLMo evidence.