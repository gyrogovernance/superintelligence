{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GyroGem Chat Training (Gemma 3 + Unsloth + LoRA)\n",
        "\n",
        "This notebook fine-tunes **two** Gemma 3 chat models sequentially using **Unsloth + TRL SFTTrainer** on the GyroGem chat dataset.\n",
        "\n",
        "Models trained:\n",
        "1. `unsloth/gemma-3-270m-it-unsloth-bnb-4bit`\n",
        "2. `unsloth/gemma-3-1b-it-bnb-4bit`\n",
        "\n",
        "Dataset path (in repo):\n",
        "`superintelligence/secret_lab_ignore/GyroGem/training/data/gyrogem_chat_qa_dataset.jsonl`\n",
        "\n",
        "Kaggle notes:\n",
        "- Enable **GPU** (prefer **P100**)\n",
        "- Enable **Internet**\n",
        "- This notebook does **not** export the dataset; it just reads it from the cloned repo.\n",
        "\n",
        "Gemma 3 precision note (Unsloth):\n",
        "- On GPUs without bfloat16 tensor cores (T4, P100, V100, etc.), Gemma 3 can overflow in float16.\n",
        "- We train with **fp16=False** and **bf16=False** (fp32 trainer args) plus gradient clipping for stability.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re, gc\n",
        "import torch\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    print(\"VRAM (GB):\", round(props.total_memory / 1024**3, 2))\n",
        "else:\n",
        "    print(\"NOTE: CUDA is not available. In Kaggle, set Accelerator=GPU (prefer P100) before training.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Install dependencies (Unsloth + TRL)\n",
        "\n",
        "We pin versions compatible with Unsloth 2026.x to avoid the dependency issues you saw.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import subprocess, re, torch\n",
        "\n",
        "def run(cmd):\n",
        "    subprocess.check_call(cmd, shell=True)\n",
        "\n",
        "run(\"pip -q install --upgrade pip\")\n",
        "\n",
        "# xformers version mapping like in Unsloth notebooks\n",
        "v = re.match(r\"[\\d]+\\.[\\d]+\", str(torch.__version__)).group(0)\n",
        "xformers = \"xformers==\" + {\"2.10\":\"0.0.34\",\"2.9\":\"0.0.33.post1\",\"2.8\":\"0.0.32.post2\"}.get(v, \"0.0.34\")\n",
        "\n",
        "# Core deps: match Unsloth's constraints\n",
        "run('pip -q install \"datasets==4.3.0\"')\n",
        "run('pip -q install \"transformers==4.56.2\"')\n",
        "run('pip -q install --no-deps \"trl==0.22.2\"')\n",
        "run('pip -q install \"peft>=0.18.0,<0.19.0\"')\n",
        "run('pip -q install \"accelerate\" \"bitsandbytes\" \"sentencepiece\" \"protobuf\" \"huggingface_hub>=0.34.0\" hf_transfer')\n",
        "\n",
        "# Extra deps Unsloth complains about if missing\n",
        "run('pip -q install \"tyro\" \"msgspec\" \"cut_cross_entropy\" \"torchao>=0.13.0\"')\n",
        "\n",
        "# Finally, install Unsloth + zoo with dependencies\n",
        "run(f'pip -q install {xformers} unsloth unsloth_zoo')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Hugging Face login (manual, toggle when you run)\n",
        "\n",
        "To avoid Kaggle auto-run breaking (no stdin), this cell does **not** log in by default.\n",
        "\n",
        "When you open the notebook yourself:\n",
        "1. Set `DO_HF_LOGIN = True`\n",
        "2. Run this cell\n",
        "3. Paste your `hf_...` token when asked\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "\n",
        "DO_HF_LOGIN = False  # <- set to True and re-run this cell when you run interactively\n",
        "\n",
        "if DO_HF_LOGIN:\n",
        "    HF_TOKEN = getpass(\"HF token: \")\n",
        "    login(token=HF_TOKEN)\n",
        "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    print(\"HF login complete.\")\n",
        "else:\n",
        "    print(\"HF login skipped. When running interactively, set DO_HF_LOGIN=True and re-run this cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Clone repo and load dataset\n",
        "\n",
        "Repo: `https://github.com/gyrogovernance/superintelligence.git`\n",
        "\n",
        "Dataset: `GyroGem/training/data/gyrogem_chat_qa_dataset.jsonl` inside `secret_lab_ignore`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/gyrogovernance/superintelligence.git\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd superintelligence/secret_lab_ignore\n",
        "repo_root = os.getcwd()\n",
        "print(\"Repo root:\", repo_root)\n",
        "\n",
        "DATA_PATH = os.path.join(repo_root, \"GyroGem\", \"training\", \"data\", \"gyrogem_chat_qa_dataset.jsonl\")\n",
        "print(\"Dataset path:\", DATA_PATH)\n",
        "assert os.path.exists(DATA_PATH), f\"Missing dataset file at {DATA_PATH}\"\n",
        "print(\"Dataset found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
        "print(\"Rows:\", len(raw))\n",
        "print(\"Columns:\", raw.column_names)\n",
        "\n",
        "ex = raw[0]\n",
        "assert \"conversations\" in ex\n",
        "assert isinstance(ex[\"conversations\"], list) and len(ex[\"conversations\"]) >= 2\n",
        "assert \"role\" in ex[\"conversations\"][0] and \"content\" in ex[\"conversations\"][0]\n",
        "print(\"Example id:\", ex.get(\"id\", \"(no id)\"))\n",
        "print(\"Turns:\", len(ex[\"conversations\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Render dataset once with Gemma 3 chat template\n",
        "\n",
        "We apply the Gemma 3 chat template to `conversations` and store the result in a `text` field.\n",
        "This rendered dataset is reused for both model sizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "RENDER_TOKENIZER_MODEL = \"unsloth/gemma-3-1b-it-bnb-4bit\"\n",
        "render_token = os.environ.get(\"HF_TOKEN\")  # if you logged in, this is set; otherwise HF cache may still work\n",
        "tok = AutoTokenizer.from_pretrained(RENDER_TOKENIZER_MODEL, token=render_token)\n",
        "tok = get_chat_template(tok, chat_template=\"gemma3\")\n",
        "\n",
        "def render_chat(examples):\n",
        "    texts = []\n",
        "    for convo in examples[\"conversations\"]:\n",
        "        t = tok.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
        "        if t.startswith(\"<bos>\"):\n",
        "            t = t[len(\"<bos>\"):]\n",
        "        texts.append(t)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = raw.map(render_chat, batched=True)\n",
        "print(\"Rendered columns:\", dataset.column_names)\n",
        "print(\"Rendered preview:\\n\", dataset[0][\"text\"][:600])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Basic length check\n",
        "\n",
        "We keep `MAX_SEQ_LENGTH` conservative for Kaggle GPUs (2048). Longer examples will be truncated by the trainer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_SEQ_LENGTH = 2048\n",
        "\n",
        "def token_len(example):\n",
        "    return {\"n_tokens\": len(tok(example[\"text\"]).input_ids)}\n",
        "\n",
        "lens = dataset.map(token_len)\n",
        "mx = max(lens[\"n_tokens\"])\n",
        "p95 = sorted(lens[\"n_tokens\"])[int(0.95 * len(lens))]\n",
        "print(\"Max tokens:\", mx)\n",
        "print(\"P95 tokens:\", p95)\n",
        "if mx > MAX_SEQ_LENGTH:\n",
        "    print(\"NOTE: Some samples exceed MAX_SEQ_LENGTH; trainer will truncate them.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Training function (Unsloth + LoRA + responses-only)\n",
        "\n",
        "We:\n",
        "- Load model (4-bit) via Unsloth\n",
        "- Add LoRA\n",
        "- Mask user/system turns (train on responses only)\n",
        "- Train with fp32 trainer args (no fp16/bf16) + gradient clipping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "from unsloth.chat_templates import get_chat_template as unsloth_get_chat_template, train_on_responses_only\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "def train_one(\n",
        "    model_name: str,\n",
        "    output_dir: str,\n",
        "    lora_r: int,\n",
        "    lora_alpha: int,\n",
        "    learning_rate: float = 2e-5,\n",
        "    num_train_epochs: float = 1.0,\n",
        "    eval_frac: float = 0.05,\n",
        "):\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Training:\", model_name)\n",
        "    print(\"Output:\", output_dir)\n",
        "    print(\"LoRA r:\", lora_r, \"alpha:\", lora_alpha)\n",
        "    print(\"==============================\\n\")\n",
        "\n",
        "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "    model, tokenizer = FastModel.from_pretrained(\n",
        "        model_name=model_name,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        load_in_4bit=True,\n",
        "        load_in_8bit=False,\n",
        "        full_finetuning=False,\n",
        "        token=hf_token,\n",
        "    )\n",
        "    tokenizer = unsloth_get_chat_template(tokenizer, chat_template=\"gemma3\")\n",
        "\n",
        "    model = FastModel.get_peft_model(\n",
        "        model,\n",
        "        r=lora_r,\n",
        "        target_modules=[\n",
        "            \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
        "            \"gate_proj\",\"up_proj\",\"down_proj\",\n",
        "        ],\n",
        "        lora_alpha=lora_alpha,\n",
        "        lora_dropout=0,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=3407,\n",
        "        use_rslora=False,\n",
        "        loftq_config=None,\n",
        "    )\n",
        "\n",
        "    ds = dataset.shuffle(seed=3407)\n",
        "    splits = ds.train_test_split(test_size=eval_frac, seed=3407)\n",
        "    train_ds = splits[\"train\"]\n",
        "    eval_ds  = splits[\"test\"]\n",
        "\n",
        "    per_device_train_batch_size = 2\n",
        "    gradient_accumulation_steps = 4\n",
        "\n",
        "    bf16 = False\n",
        "    fp16 = False\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=eval_ds,\n",
        "        args=SFTConfig(\n",
        "            dataset_text_field=\"text\",\n",
        "            max_seq_length=MAX_SEQ_LENGTH,\n",
        "            packing=True,\n",
        "            per_device_train_batch_size=per_device_train_batch_size,\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            learning_rate=learning_rate,\n",
        "            warmup_steps=5,\n",
        "            logging_steps=10,\n",
        "            evaluation_strategy=\"steps\",\n",
        "            eval_steps=50,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=50,\n",
        "            optim=\"adamw_8bit\",\n",
        "            weight_decay=0.001,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            max_grad_norm=1.0,\n",
        "            seed=3407,\n",
        "            output_dir=output_dir,\n",
        "            report_to=\"none\",\n",
        "            bf16=bf16,\n",
        "            fp16=fp16,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    trainer = train_on_responses_only(\n",
        "        trainer,\n",
        "        instruction_part=\"<start_of_turn>user\\n\",\n",
        "        response_part=\"<start_of_turn>model\\n\",\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(\"Saved LoRA adapters to:\", output_dir)\n",
        "\n",
        "    del trainer, model, tokenizer\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return output_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Train model A: Gemma 3 270m\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_270M = \"unsloth/gemma-3-270m-it-unsloth-bnb-4bit\"\n",
        "OUT_270M = \"/kaggle/working/gyrogem_gemma3_270m_lora\"\n",
        "\n",
        "train_one(\n",
        "    model_name=MODEL_270M,\n",
        "    output_dir=OUT_270M,\n",
        "    lora_r=32,\n",
        "    lora_alpha=32,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=1.0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Train model B: Gemma 3 1B\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_1B = \"unsloth/gemma-3-1b-it-bnb-4bit\"\n",
        "OUT_1B = \"/kaggle/working/gyrogem_gemma3_1b_lora\"\n",
        "\n",
        "train_one(\n",
        "    model_name=MODEL_1B,\n",
        "    output_dir=OUT_1B,\n",
        "    lora_r=64,\n",
        "    lora_alpha=64,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=1.0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Optional inference sanity checks (after training)\n",
        "\n",
        "Uses Gemma team’s recommended inference settings:\n",
        "- temperature=1.0, top_k=64, top_p=0.95, repetition_penalty=1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "from peft import PeftModel\n",
        "from unsloth import FastModel\n",
        "from unsloth.chat_templates import get_chat_template as unsloth_get_chat_template\n",
        "\n",
        "def quick_infer(base_model_name, adapter_dir, prompt):\n",
        "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "    model, tokenizer = FastModel.from_pretrained(\n",
        "        model_name=base_model_name,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        load_in_4bit=True,\n",
        "        load_in_8bit=False,\n",
        "        full_finetuning=False,\n",
        "        token=hf_token,\n",
        "    )\n",
        "    tokenizer = unsloth_get_chat_template(tokenizer, chat_template=\"gemma3\")\n",
        "\n",
        "    model = PeftModel.from_pretrained(model, adapter_dir)\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    if text.startswith(\"<bos>\"):\n",
        "        text = text[len(\"<bos>\"):]\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    print(\"\\n---\", base_model_name, \"with\", adapter_dir, \"---\")\n",
        "    _ = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=220,\n",
        "        temperature=1.0,\n",
        "        top_k=64,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.0,\n",
        "        streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
        "    )\n",
        "\n",
        "    del model, tokenizer\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "prompt = \"What is ✋ The Human Mark (THM) in simple terms?\"\n",
        "# Run these after training if you want to inspect behaviour\n",
        "# quick_infer(MODEL_270M, OUT_270M, prompt)\n",
        "# quick_infer(MODEL_1B, OUT_1B, prompt)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}